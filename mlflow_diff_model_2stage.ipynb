{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d8ad028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "from PIL import Image \n",
    "from pathlib import Path\n",
    "import visualkeras\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from tensorflow.keras.applications import mobilenet_v3, efficientnet\n",
    "import itertools # ç¢ºä¿ itertools å·²åŒ¯å…¥\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers, layers, models\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint \n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b5b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 1. å»ºç«‹æ¨¡å‹çš„å‡½å¼ (å·²æ›´æ–°ï¼Œæ”¯æ´å…©éšæ®µè¨“ç·´çš„å‡çµæ§åˆ¶)\n",
    "def build_model(model_name, learning_rate, dropout_rate, dense_units, trainable_layers, num_classes, freeze_base_model=False):\n",
    "    \"\"\"\n",
    "    æ ¹æ“šå‚³å…¥çš„è¶…åƒæ•¸å»ºç«‹ä¸¦ç·¨è­¯æŒ‡å®šçš„ Keras æ¨¡å‹ã€‚\n",
    "    *** ä½¿ç”¨ Functional API ä»¥ç¢ºä¿æ¨¡å‹çµæ§‹çš„ç©©å¥æ€§ ***\n",
    "    \"\"\"\n",
    "    input_shape = (224, 224, 3)\n",
    "    \n",
    "    # å®šç¾©æ¨¡å‹è¼¸å…¥\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # æ ¹æ“š model_name é¸æ“‡ä¸¦å»ºç«‹åŸºç¤æ¨¡å‹\n",
    "    if model_name == 'MobileNetV3-Large':\n",
    "        base_model = keras.applications.MobileNetV3Large(\n",
    "            input_shape=input_shape,\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            # é—œéµï¼šå°‡ base_model çš„è¼¸å…¥èˆ‡æˆ‘å€‘å®šç¾©çš„ `inputs` å¼µé‡é€£æ¥èµ·ä¾†\n",
    "            input_tensor=inputs \n",
    "        )\n",
    "        print(\"âœ… Base model loaded: MobileNetV3-Large\")\n",
    "    elif model_name == 'EfficientNet-B0':\n",
    "        base_model = keras.applications.EfficientNetB0(\n",
    "            input_shape=input_shape,\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            # é—œéµï¼šå°‡ base_model çš„è¼¸å…¥èˆ‡æˆ‘å€‘å®šç¾©çš„ `inputs` å¼µé‡é€£æ¥èµ·ä¾†\n",
    "            input_tensor=inputs\n",
    "        )\n",
    "        print(\"âœ… Base model loaded: EfficientNetB0\")\n",
    "    else:\n",
    "        raise ValueError(f\"ä¸æ”¯æ´çš„æ¨¡å‹åç¨±: {model_name}ã€‚\")\n",
    "        \n",
    "    # --- è¨­å®šåŸºåº•æ¨¡å‹çš„å¯è¨“ç·´æ€§ ---\n",
    "    if freeze_base_model:\n",
    "        base_model.trainable = False\n",
    "        print(\"ğŸ§Š Stage 1: Base model is FROZEN.\")\n",
    "    else:\n",
    "        base_model.trainable = True\n",
    "        if trainable_layers > 0 and trainable_layers < len(base_model.layers):\n",
    "            for layer in base_model.layers[:-trainable_layers]:\n",
    "                layer.trainable = False\n",
    "            print(f\"ğŸ”¥ Stage 2: Fine-tuning. Last {trainable_layers} layers of base model are UNFROZEN.\")\n",
    "        else:\n",
    "            print(\"ğŸ”¥ Stage 2: Fine-tuning. All layers of base model are UNFROZEN.\")\n",
    "\n",
    "    # --- ä½¿ç”¨ Functional API ä¸²æ¥æ¨¡å‹ ---\n",
    "    # å–å¾— base_model çš„è¼¸å‡º\n",
    "    x = base_model.output\n",
    "    # é€£æ¥åˆ°å¾ŒçºŒçš„å±¤\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    # è¼¸å‡ºå±¤\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # å»ºç«‹æœ€çµ‚æ¨¡å‹ï¼Œæ˜ç¢ºæŒ‡å®šè¼¸å…¥å’Œè¼¸å‡º\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # ç·¨è­¯æ¨¡å‹ (éœ€è¦ä½¿ç”¨ Adam çš„å¯¦ä¾‹ï¼Œè€Œä¸æ˜¯å­—ä¸² 'Adam')\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b38f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MLflow è¦–è¦ºåŒ–èˆ‡æ—¥èªŒè¨˜éŒ„è¼”åŠ©å‡½å¼ (ç¶­æŒä¸è®Š)\n",
    "\n",
    "def plot_and_log_history(history, filename=\"history_plots.png\"):\n",
    "    \"\"\"\n",
    "    ç¹ªè£½æº–ç¢ºç‡å’Œæå¤±å‡½æ•¸çš„æ­·å²æ›²ç·šï¼Œä¸¦å°‡å…¶å„²å­˜ç‚ºåœ–ç‰‡ï¼Œæœ€å¾Œè¨˜éŒ„åˆ° MLflowã€‚\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # æº–ç¢ºç‡åœ–\n",
    "    ax1.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.set_title('Accuracy over epochs')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # æå¤±å‡½æ•¸åœ–\n",
    "    ax2.plot(history.history['loss'], label='Train Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Val Loss')\n",
    "    ax2.set_title('Loss over epochs')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # å„²å­˜åœ–ç‰‡ä¸¦é—œé–‰ç¹ªåœ–\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)  # ç¢ºä¿é—œé–‰æ­£ç¢ºçš„ figure\n",
    "    \n",
    "    # å°‡åœ–ç‰‡è¨˜éŒ„åˆ° MLflow\n",
    "    mlflow.log_artifact(filename)\n",
    "    print(f\"âœ… '{filename}' has been logged to MLflow.\")\n",
    "\n",
    "def log_confusion_matrix(y_true, y_pred, class_labels, filename=\"confusion_matrix.png\"):\n",
    "    \"\"\" ç¹ªè£½ã€å„²å­˜ä¸¦è¨˜éŒ„æ¨™æº–æ··æ·†çŸ©é™£ \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(18, 14)) # å¢åŠ åœ–ç‰‡å¤§å°ä»¥å®¹ç´æ›´å¤šé¡åˆ¥\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels, cmap='Blues')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(filename)\n",
    "    print(f\"âœ… '{filename}' has been logged to MLflow.\")\n",
    "\n",
    "def plot_and_log_sorted_confusion_bar(y_true, y_pred, class_labels, filename=\"sorted_correct_counts.png\"):\n",
    "    \"\"\" è¨ˆç®—ã€ç¹ªè£½ä¸¦è¨˜éŒ„ä¸€å€‹é•·æ¢åœ–ï¼Œé¡¯ç¤ºæ¯å€‹é¡åˆ¥çš„æ­£ç¢ºé æ¸¬æ•¸é‡ï¼ˆç”±é«˜åˆ°ä½æ’åºï¼‰ã€‚ \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    correct_counts = np.diag(cm)\n",
    "    \n",
    "    sorted_idx = np.argsort(correct_counts)[::-1]\n",
    "    sorted_counts = correct_counts[sorted_idx]\n",
    "    sorted_labels = np.array(class_labels)[sorted_idx]\n",
    "\n",
    "    plt.figure(figsize=(18, 8)) # å¢åŠ åœ–ç‰‡å¯¬åº¦\n",
    "    plt.bar(range(len(sorted_counts)), sorted_counts, color='lightgreen')\n",
    "    plt.xticks(range(len(sorted_labels)), sorted_labels, rotation=90)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Correctly Predicted Count')\n",
    "    plt.title('Correctly Predicted Count per Class (sorted)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "    mlflow.log_artifact(filename)\n",
    "    print(f\"âœ… '{filename}' has been logged to MLflow.\")\n",
    "\n",
    "def evaluate_and_log_all_reports(model, data_generator, class_labels):\n",
    "    \"\"\"\n",
    "    è©•ä¼°æ¨¡å‹ä¸¦è¨˜éŒ„åˆ†é¡å ±å‘Šã€æ··æ·†çŸ©é™£åŠå…¶ä»–åˆ†æåœ–åˆ° MLflowã€‚\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*20 + \" Starting Full Evaluation for Logging \" + \"=\"*20)\n",
    "\n",
    "    # é—œéµï¼šé‡ç½® generator ä»¥ç¢ºä¿é †åºæ­£ç¢º\n",
    "    data_generator.reset()\n",
    "\n",
    "    # 1. æ¨¡å‹é æ¸¬\n",
    "    # ä½¿ç”¨ predict() æ–¹æ³•ï¼Œä¸¦æŒ‡å®š stepsï¼Œå¯ä»¥ç¢ºä¿è™•ç†å®Œæ‰€æœ‰æ¨£æœ¬\n",
    "    predictions = model.predict(data_generator, steps=len(data_generator), verbose=1)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # 2. å–å¾—çœŸå¯¦æ¨™ç±¤\n",
    "    # æ³¨æ„ï¼šgenerator çš„ classes å±¬æ€§åŒ…å«äº†æ‰€æœ‰æ¨£æœ¬çš„çœŸå¯¦æ¨™ç±¤\n",
    "    y_true = data_generator.classes\n",
    "\n",
    "    # ç¢ºä¿é•·åº¦ä¸€è‡´ï¼Œé€™åœ¨ generator batch_size ç„¡æ³•æ•´é™¤ç¸½æ¨£æœ¬æ•¸æ™‚å¾ˆé‡è¦\n",
    "    if len(y_pred) != len(y_true):\n",
    "        print(f\"âš ï¸ y_pred length {len(y_pred)} vs y_true length {len(y_true)}. This is normal if batch_size doesn't divide total samples.\")\n",
    "        # `predict` æœƒè™•ç†å®Œæ‰€æœ‰æ¨£æœ¬ï¼Œæ‰€ä»¥ y_true ä¹Ÿæ‡‰è©²æ˜¯å®Œæ•´çš„\n",
    "        y_true = y_true[:len(y_pred)]\n",
    "\n",
    "    # 3. ç”¢ç”Ÿä¸¦è¨˜éŒ„åˆ†é¡å ±å‘Š\n",
    "    report = classification_report(y_true, y_pred, target_names=class_labels, digits=4, zero_division=0)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "    mlflow.log_text(report, \"classification_report.txt\")\n",
    "    print(\"âœ… 'classification_report.txt' has been logged to MLflow.\")\n",
    "\n",
    "    # 4. è¨˜éŒ„æ··æ·†çŸ©é™£\n",
    "    log_confusion_matrix(y_true, y_pred, class_labels)\n",
    "\n",
    "    # 5. è¨˜éŒ„æ’åºå¾Œçš„æ­£ç¢ºé æ¸¬æ•¸é‡é•·æ¢åœ–\n",
    "    plot_and_log_sorted_confusion_bar(y_true, y_pred, class_labels)\n",
    "    \n",
    "    print(\"=\"*20 + \" Full Evaluation Logging Complete \" + \"=\"*20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ad3f6",
   "metadata": {},
   "source": [
    "### å…©ç¨®æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b30e6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 2. è‡ªå‹•è¨“ç·´èˆ‡èª¿åƒçš„ä¸»å‡½å¼ (å·²æ›´æ–°ç‚ºä½¿ç”¨ preprocess_input)\n",
    "def train_and_tune(train_path, validation_path, target_accuracy):\n",
    "    \"\"\"\n",
    "    è‡ªå‹•åŸ·è¡Œå…©éšæ®µè¨“ç·´å’Œè¶…åƒæ•¸èª¿æ•´ã€‚\n",
    "    ç‚ºæ¯å€‹æ¨¡å‹å‹•æ…‹å‰µå»ºä½¿ç”¨å…¶å°ˆå±¬ preprocess_input çš„æ•¸æ“šç”Ÿæˆå™¨ã€‚\n",
    "    \"\"\"\n",
    "    # å®šç¾©è¶…åƒæ•¸çš„æœå°‹ç©ºé–“\n",
    "    param_space = {\n",
    "        'model_name': [\n",
    "            'MobileNetV3-Large', \n",
    "            'EfficientNet-B0'],\n",
    "        'learning_rate': [0.001],\n",
    "        'dropout_rate': [0.3],\n",
    "        'dense_units': [256],\n",
    "        'trainable_layers': [80]  # å¯èª¿æ•´çš„å±¤æ•¸\n",
    "    }\n",
    "    \n",
    "    STAGE_1_EPOCHS = 20\n",
    "    STAGE_2_EPOCHS = 50\n",
    "    FINE_TUNE_LR_MULTIPLIER = 0.1\n",
    "    \n",
    "    keys, values = zip(*param_space.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    best_run_id = None\n",
    "\n",
    "    mlflow.set_experiment(\"MobileNetV3 & EfficientNet-B0 2éšæ®µè¨“ç·´  è³‡æ–™é›†2(60)\")\n",
    "    \n",
    "    for i, params in enumerate(param_combinations):\n",
    "        if best_val_accuracy >= target_accuracy:\n",
    "            print(f\"ğŸ¯ ç›®æ¨™å·²é”æˆ (Accuracy >= {target_accuracy:.2f})ã€‚åœæ­¢æœå°‹ã€‚\")\n",
    "            break\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"ğŸš€ Run {i+1}/{len(param_combinations)}: å˜—è©¦åƒæ•¸çµ„åˆ: {params}\")\n",
    "\n",
    "        # --- é—œéµä¿®æ”¹ï¼šæ ¹æ“šæ¨¡å‹åç¨±é¸æ“‡é è™•ç†å‡½å¼ ---\n",
    "        if params['model_name'] == 'MobileNetV3-Large':\n",
    "            preprocessing_function = mobilenet_v3.preprocess_input\n",
    "            print(\"INFO: Using MobileNetV3 preprocess_input.\")\n",
    "        elif params['model_name'] == 'EfficientNet-B0':\n",
    "            preprocessing_function = efficientnet.preprocess_input\n",
    "            print(\"INFO: Using EfficientNet preprocess_input.\")\n",
    "        else:\n",
    "            raise ValueError(f\"æœªçŸ¥çš„æ¨¡å‹: {params['model_name']}\")\n",
    "\n",
    "        # --- åœ¨è¿´åœˆå…§éƒ¨å‰µå»ºæ•¸æ“šç”Ÿæˆå™¨ ---\n",
    "        train_datagen_aug = ImageDataGenerator(\n",
    "            preprocessing_function=preprocessing_function,  # ä½¿ç”¨æŒ‡å®šçš„å‡½å¼\n",
    "            rotation_range=15,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            brightness_range=[0.9, 1.1],\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "        \n",
    "        validation_datagen = ImageDataGenerator(preprocessing_function=preprocessing_function)\n",
    "\n",
    "        train_gen = train_datagen_aug.flow_from_directory(\n",
    "            train_path,\n",
    "            target_size=(224, 224),\n",
    "            batch_size=32,\n",
    "            class_mode='categorical',\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        val_gen = validation_datagen.flow_from_directory(\n",
    "            validation_path,\n",
    "            target_size=(224, 224),\n",
    "            batch_size=32,\n",
    "            class_mode='categorical',\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # å¾ç”Ÿæˆå™¨ç²å–æ•¸æ“šé›†ä¿¡æ¯\n",
    "        num_classes = train_gen.num_classes\n",
    "        class_labels = list(train_gen.class_indices.keys())\n",
    "\n",
    "        # å„²å­˜é¡åˆ¥åç¨±åˆ° classes.csv\n",
    "        with open('classes_new.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for cls in class_labels:\n",
    "                writer.writerow([cls])\n",
    "        \n",
    "        # è¨ˆç®— class_weight\n",
    "        class_indices = np.unique(train_gen.classes)\n",
    "        class_weights_array = compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=class_indices,\n",
    "            y=train_gen.classes\n",
    "        )\n",
    "        class_weight = dict(zip(class_indices, class_weights_array))\n",
    "\n",
    "        # --- è¨“ç·´æµç¨‹ï¼ˆèˆ‡ä¹‹å‰ç›¸åŒï¼‰ ---\n",
    "        with mlflow.start_run(run_name=f\"{params['model_name']}_run_{i+1}\") as run:\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-8)\n",
    "            ]\n",
    "            \n",
    "            # STAGE 1: è¨“ç·´åˆ†é¡é ­\n",
    "            print(\"\\n\" + \"-\" * 20 + \" STAGE 1: Feature Extraction \" + \"-\" * 20)\n",
    "            model = build_model(\n",
    "                **params, \n",
    "                num_classes=num_classes, \n",
    "                freeze_base_model=True\n",
    "                )\n",
    "            \n",
    "            history_stage1 = model.fit(\n",
    "                train_gen, \n",
    "                epochs=STAGE_1_EPOCHS, \n",
    "                validation_data=val_gen, \n",
    "                callbacks=callbacks, \n",
    "                class_weight=class_weight, \n",
    "                verbose=1\n",
    "                )\n",
    "            \n",
    "            model.save_weights(f\"checkpoints/stage1/{i+1}stage1.weights.h5\")\n",
    "\n",
    "            # STAGE 2: å¾®èª¿\n",
    "            print(\"\\n\" + \"-\" * 20 + \" STAGE 2: Fine-Tuning \" + \"-\" * 20)\n",
    "\n",
    "            fine_tune_lr = params['learning_rate'] * FINE_TUNE_LR_MULTIPLIER\n",
    "            params['learning_rate'] = fine_tune_lr\n",
    "            model = build_model(\n",
    "                **params, \n",
    "                num_classes=num_classes, \n",
    "                freeze_base_model=False\n",
    "                )\n",
    "            \n",
    "            model.load_weights(f\"checkpoints/stage1/{i+1}stage1.weights.h5\")\n",
    "            \n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-9),\n",
    "                ModelCheckpoint(\"model_checkpoint.keras\", monitor=\"val_loss\", save_best_only=True)\n",
    "            ]\n",
    "            history_stage2 = model.fit(train_gen, epochs=STAGE_2_EPOCHS, validation_data=val_gen, class_weight=class_weight, callbacks=callbacks, verbose=1)\n",
    "            \n",
    "            # MLflow è¨˜éŒ„\n",
    "            full_history = {}\n",
    "            for key in history_stage1.history.keys():\n",
    "                full_history[key] = history_stage1.history[key] + history_stage2.history[key]\n",
    "            \n",
    "            val_accuracy = max(history_stage2.history['val_accuracy'])\n",
    "            mlflow.log_metric(\"best_val_accuracy\", val_accuracy)\n",
    "            \n",
    "            plot_and_log_history(type('History', (), {'history': full_history})())\n",
    "            evaluate_and_log_all_reports(model, val_gen, class_labels)\n",
    "            mlflow.keras.log_model(model, \"model\")\n",
    "            \n",
    "            print(f\"âœ”ï¸ Run {run.info.run_id} å®Œæˆã€‚æœ€ä½³é©—è­‰æº–ç¢ºç‡: {val_accuracy:.4f}\")\n",
    "            \n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_run_id = run.info.run_id\n",
    "                print(f\"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! Model: {params['model_name']}, Run ID: {best_run_id}, Accuracy: {best_val_accuracy:.4f}\")\n",
    "                model.save(f'checkpoints/bestmodel/{i+1}best_tuned_model.keras')\n",
    "                print(f\"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²æ›´æ–°ä¸¦å„²å­˜ç‚º '{i+1}best_tuned_model.h5'\")\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸ è‡ªå‹•èª¿åƒå®Œæˆã€‚\")\n",
    "    if best_run_id:\n",
    "        print(f\"ğŸ† æœ€çµ‚æœ€ä½³æ¨¡å‹çš„ Run ID ç‚º: {best_run_id}\")\n",
    "        print(f\"ğŸ† æœ€çµ‚æœ€ä½³é©—è­‰æº–ç¢ºç‡ç‚º: {best_val_accuracy:.4f}\")\n",
    "    else:\n",
    "        print(\"âŒ æœªèƒ½æˆåŠŸè¨“ç·´ä»»ä½•æ¨¡å‹ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e09c9c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸš€ Run 1/2: å˜—è©¦åƒæ•¸çµ„åˆ: {'model_name': 'MobileNetV3-Large', 'learning_rate': 0.001, 'dropout_rate': 0.3, 'dense_units': 256, 'trainable_layers': 80}\n",
      "INFO: Using MobileNetV3 preprocess_input.\n",
      "Found 26307 images belonging to 60 classes.\n",
      "Found 7514 images belonging to 60 classes.\n",
      "\n",
      "-------------------- STAGE 1: Feature Extraction --------------------\n",
      "âœ… Base model loaded: MobileNetV3-Large\n",
      "ğŸ§Š Stage 1: Base model is FROZEN.\n",
      "Epoch 1/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1331s\u001b[0m 2s/step - accuracy: 0.4438 - loss: 2.0003 - val_accuracy: 0.6801 - val_loss: 1.1039 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1169s\u001b[0m 1s/step - accuracy: 0.6121 - loss: 1.2995 - val_accuracy: 0.7072 - val_loss: 0.9815 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1100s\u001b[0m 1s/step - accuracy: 0.6599 - loss: 1.1400 - val_accuracy: 0.7111 - val_loss: 0.9508 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1297s\u001b[0m 2s/step - accuracy: 0.6911 - loss: 1.0359 - val_accuracy: 0.7372 - val_loss: 0.8799 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1457s\u001b[0m 2s/step - accuracy: 0.7044 - loss: 0.9789 - val_accuracy: 0.7515 - val_loss: 0.8330 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1224s\u001b[0m 1s/step - accuracy: 0.7270 - loss: 0.9123 - val_accuracy: 0.7490 - val_loss: 0.8350 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1355s\u001b[0m 2s/step - accuracy: 0.7337 - loss: 0.8737 - val_accuracy: 0.7554 - val_loss: 0.8121 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1429s\u001b[0m 2s/step - accuracy: 0.7422 - loss: 0.8450 - val_accuracy: 0.7683 - val_loss: 0.7815 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1229s\u001b[0m 1s/step - accuracy: 0.7521 - loss: 0.8220 - val_accuracy: 0.7537 - val_loss: 0.8234 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1324s\u001b[0m 2s/step - accuracy: 0.7604 - loss: 0.7902 - val_accuracy: 0.7715 - val_loss: 0.7785 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1522s\u001b[0m 2s/step - accuracy: 0.7653 - loss: 0.7721 - val_accuracy: 0.7716 - val_loss: 0.7855 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1460s\u001b[0m 2s/step - accuracy: 0.7741 - loss: 0.7379 - val_accuracy: 0.7759 - val_loss: 0.7669 - learning_rate: 0.0010\n",
      "Epoch 13/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1492s\u001b[0m 2s/step - accuracy: 0.7775 - loss: 0.7291 - val_accuracy: 0.7718 - val_loss: 0.7782 - learning_rate: 0.0010\n",
      "Epoch 14/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1460s\u001b[0m 2s/step - accuracy: 0.7783 - loss: 0.7288 - val_accuracy: 0.7715 - val_loss: 0.7722 - learning_rate: 0.0010\n",
      "Epoch 15/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1496s\u001b[0m 2s/step - accuracy: 0.7844 - loss: 0.6991 - val_accuracy: 0.7840 - val_loss: 0.7623 - learning_rate: 0.0010\n",
      "Epoch 16/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1493s\u001b[0m 2s/step - accuracy: 0.7901 - loss: 0.6880 - val_accuracy: 0.7815 - val_loss: 0.7629 - learning_rate: 0.0010\n",
      "Epoch 17/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1495s\u001b[0m 2s/step - accuracy: 0.7926 - loss: 0.6733 - val_accuracy: 0.7849 - val_loss: 0.7511 - learning_rate: 0.0010\n",
      "Epoch 18/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1524s\u001b[0m 2s/step - accuracy: 0.7963 - loss: 0.6702 - val_accuracy: 0.7832 - val_loss: 0.7522 - learning_rate: 0.0010\n",
      "Epoch 19/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1499s\u001b[0m 2s/step - accuracy: 0.8006 - loss: 0.6573 - val_accuracy: 0.7762 - val_loss: 0.7714 - learning_rate: 0.0010\n",
      "Epoch 20/20\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1520s\u001b[0m 2s/step - accuracy: 0.8016 - loss: 0.6556 - val_accuracy: 0.7811 - val_loss: 0.7631 - learning_rate: 0.0010\n",
      "\n",
      "-------------------- STAGE 2: Fine-Tuning --------------------\n",
      "âœ… Base model loaded: MobileNetV3-Large\n",
      "ğŸ”¥ Stage 2: Fine-tuning. Last 80 layers of base model are UNFROZEN.\n",
      "Epoch 1/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1819s\u001b[0m 2s/step - accuracy: 0.7435 - loss: 0.8891 - val_accuracy: 0.7836 - val_loss: 0.7488 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1735s\u001b[0m 2s/step - accuracy: 0.8163 - loss: 0.6068 - val_accuracy: 0.8203 - val_loss: 0.6231 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1727s\u001b[0m 2s/step - accuracy: 0.8463 - loss: 0.5000 - val_accuracy: 0.8410 - val_loss: 0.5692 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1774s\u001b[0m 2s/step - accuracy: 0.8644 - loss: 0.4338 - val_accuracy: 0.8504 - val_loss: 0.5339 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1290s\u001b[0m 2s/step - accuracy: 0.8797 - loss: 0.3897 - val_accuracy: 0.8523 - val_loss: 0.5298 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1214s\u001b[0m 1s/step - accuracy: 0.8907 - loss: 0.3532 - val_accuracy: 0.8557 - val_loss: 0.5176 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1212s\u001b[0m 1s/step - accuracy: 0.8997 - loss: 0.3179 - val_accuracy: 0.8575 - val_loss: 0.5411 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1214s\u001b[0m 1s/step - accuracy: 0.9077 - loss: 0.2952 - val_accuracy: 0.8641 - val_loss: 0.5077 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1252s\u001b[0m 2s/step - accuracy: 0.9173 - loss: 0.2625 - val_accuracy: 0.8689 - val_loss: 0.4927 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1220s\u001b[0m 1s/step - accuracy: 0.9223 - loss: 0.2460 - val_accuracy: 0.8740 - val_loss: 0.4920 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1219s\u001b[0m 1s/step - accuracy: 0.9291 - loss: 0.2254 - val_accuracy: 0.8712 - val_loss: 0.5122 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1213s\u001b[0m 1s/step - accuracy: 0.9333 - loss: 0.2109 - val_accuracy: 0.8746 - val_loss: 0.4899 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m917s\u001b[0m 1s/step - accuracy: 0.9379 - loss: 0.1937 - val_accuracy: 0.8760 - val_loss: 0.5380 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m828s\u001b[0m 1s/step - accuracy: 0.9410 - loss: 0.1890 - val_accuracy: 0.8781 - val_loss: 0.5009 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m820s\u001b[0m 996ms/step - accuracy: 0.9475 - loss: 0.1671 - val_accuracy: 0.8745 - val_loss: 0.5198 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m808s\u001b[0m 982ms/step - accuracy: 0.9561 - loss: 0.1380 - val_accuracy: 0.8836 - val_loss: 0.4898 - learning_rate: 2.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m828s\u001b[0m 941ms/step - accuracy: 0.9602 - loss: 0.1241 - val_accuracy: 0.8865 - val_loss: 0.4814 - learning_rate: 2.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m818s\u001b[0m 993ms/step - accuracy: 0.9616 - loss: 0.1198 - val_accuracy: 0.8905 - val_loss: 0.4758 - learning_rate: 2.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m800s\u001b[0m 972ms/step - accuracy: 0.9634 - loss: 0.1120 - val_accuracy: 0.8907 - val_loss: 0.4761 - learning_rate: 2.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m824s\u001b[0m 999ms/step - accuracy: 0.9666 - loss: 0.1069 - val_accuracy: 0.8894 - val_loss: 0.4776 - learning_rate: 2.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m819s\u001b[0m 995ms/step - accuracy: 0.9683 - loss: 0.0987 - val_accuracy: 0.8879 - val_loss: 0.4827 - learning_rate: 2.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m811s\u001b[0m 986ms/step - accuracy: 0.9697 - loss: 0.0929 - val_accuracy: 0.8914 - val_loss: 0.4771 - learning_rate: 4.0000e-06\n",
      "Epoch 23/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m810s\u001b[0m 984ms/step - accuracy: 0.9691 - loss: 0.0956 - val_accuracy: 0.8915 - val_loss: 0.4750 - learning_rate: 4.0000e-06\n",
      "Epoch 24/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m812s\u001b[0m 987ms/step - accuracy: 0.9740 - loss: 0.0844 - val_accuracy: 0.8925 - val_loss: 0.4767 - learning_rate: 4.0000e-06\n",
      "Epoch 25/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m812s\u001b[0m 987ms/step - accuracy: 0.9716 - loss: 0.0885 - val_accuracy: 0.8919 - val_loss: 0.4765 - learning_rate: 4.0000e-06\n",
      "Epoch 26/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m806s\u001b[0m 979ms/step - accuracy: 0.9700 - loss: 0.0912 - val_accuracy: 0.8918 - val_loss: 0.4777 - learning_rate: 4.0000e-06\n",
      "Epoch 27/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m809s\u001b[0m 982ms/step - accuracy: 0.9710 - loss: 0.0877 - val_accuracy: 0.8923 - val_loss: 0.4786 - learning_rate: 8.0000e-07\n",
      "Epoch 28/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m810s\u001b[0m 985ms/step - accuracy: 0.9706 - loss: 0.0900 - val_accuracy: 0.8914 - val_loss: 0.4787 - learning_rate: 8.0000e-07\n",
      "Epoch 29/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m813s\u001b[0m 987ms/step - accuracy: 0.9726 - loss: 0.0851 - val_accuracy: 0.8915 - val_loss: 0.4793 - learning_rate: 8.0000e-07\n",
      "Epoch 30/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m866s\u001b[0m 993ms/step - accuracy: 0.9727 - loss: 0.0876 - val_accuracy: 0.8919 - val_loss: 0.4793 - learning_rate: 1.6000e-07\n",
      "Epoch 31/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m822s\u001b[0m 998ms/step - accuracy: 0.9732 - loss: 0.0835 - val_accuracy: 0.8922 - val_loss: 0.4794 - learning_rate: 1.6000e-07\n",
      "Epoch 32/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m817s\u001b[0m 993ms/step - accuracy: 0.9722 - loss: 0.0855 - val_accuracy: 0.8918 - val_loss: 0.4793 - learning_rate: 1.6000e-07\n",
      "Epoch 33/50\n",
      "\u001b[1m823/823\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m813s\u001b[0m 988ms/step - accuracy: 0.9716 - loss: 0.0853 - val_accuracy: 0.8921 - val_loss: 0.4791 - learning_rate: 3.2000e-08\n",
      "âœ… 'history_plots.png' has been logged to MLflow.\n",
      "\n",
      "==================== Starting Full Evaluation for Logging ====================\n",
      "\u001b[1m235/235\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 312ms/step\n",
      "\n",
      "Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "Agaricus lemaneiformis     0.8473    0.9250    0.8845       120\n",
      "              Amaranth     0.9153    0.8780    0.8963       123\n",
      "             Baby Corn     0.9079    0.9262    0.9169       149\n",
      "         Bamboo shoots     0.9016    0.9167    0.9091       120\n",
      "                 Basil     0.8699    0.8992    0.8843       119\n",
      "           Beef Tomato     0.9355    0.9667    0.9508       120\n",
      "           Bell pepper     0.9576    0.9496    0.9536       119\n",
      "   Big Chinese Cabbage     0.7833    0.7833    0.7833       120\n",
      "          Big cucumber     0.8151    0.8083    0.8117       120\n",
      "              Bok Choy     0.8974    0.8750    0.8861       120\n",
      "       Chinese Cabbage     0.8803    0.8583    0.8692       120\n",
      "        Chinese chives     0.8807    0.8000    0.8384       120\n",
      "         Chrysanthemum     0.9009    0.8333    0.8658       120\n",
      "              Cucumber     0.8929    0.8333    0.8621       120\n",
      "          French beans     0.7955    0.8077    0.8015       130\n",
      "                Garlic     0.9384    0.9384    0.9384       146\n",
      "        Garlic sprouts     0.7519    0.8083    0.7791       120\n",
      "        Green Broccoli     0.9276    0.9527    0.9400       148\n",
      "   Green bamboo shoots     0.9590    0.9750    0.9669       120\n",
      "          Green pepper     0.9344    0.9500    0.9421       120\n",
      "                  Kale     0.9140    0.7083    0.7981       120\n",
      "               Lettuce     0.9180    0.9333    0.9256       120\n",
      "                Loofah     0.8870    0.8500    0.8681       120\n",
      "            Lotus root     0.9407    0.9250    0.9328       120\n",
      "         Mainland girl     0.8527    0.9167    0.8835       120\n",
      "   Momordica charantia     0.9561    0.9083    0.9316       120\n",
      "           Mountain Su     0.8889    0.9333    0.9106       120\n",
      "                  Okra     0.9336    0.9426    0.9381       209\n",
      "          Red broccoli     0.9652    0.9250    0.9447       120\n",
      "               Romaine     0.9231    0.9000    0.9114       120\n",
      "              Shallots     0.9115    0.8583    0.8841       120\n",
      "             Sweet Pea     0.8231    0.8917    0.8560       120\n",
      "   Sweet potato leaves     0.7985    0.8917    0.8425       120\n",
      "                  Taro     0.9055    0.9583    0.9312       120\n",
      "           WaWa dishes     0.8217    0.8833    0.8514       120\n",
      "            Water Lily     0.9752    0.9833    0.9793       120\n",
      "         Water spinach     0.8120    0.8640    0.8372       125\n",
      "          White radish     0.9394    0.9538    0.9466       195\n",
      "          Winter melon     0.8595    0.8667    0.8631       120\n",
      "                   Yam     0.9406    0.7917    0.8597       120\n",
      "             asparagus     0.9458    0.9181    0.9318       171\n",
      "               brocoli     0.9565    0.9167    0.9362       120\n",
      "               cabbage     0.8780    0.9000    0.8889       120\n",
      "                carrot     0.9355    0.9667    0.9508       120\n",
      "                celery     0.8730    0.9167    0.8943       120\n",
      "                 chili     0.9250    0.9250    0.9250       120\n",
      "             coriander     0.8938    0.8417    0.8670       120\n",
      "                  corn     0.8605    0.9250    0.8916       120\n",
      "                cowpea     0.8529    0.7250    0.7838       120\n",
      "              eggplant     0.9829    0.9583    0.9705       120\n",
      "                ginger     0.8915    0.9583    0.9237       120\n",
      "           green onion     0.8205    0.8000    0.8101       120\n",
      "                 onion     0.8661    0.9167    0.8907       120\n",
      "                   pea     0.8618    0.8833    0.8724       120\n",
      "                potato     0.8385    0.9083    0.8720       120\n",
      "               pumpkin     0.9735    0.9167    0.9442       120\n",
      "                  rape     0.7323    0.7750    0.7530       120\n",
      "               spinach     0.8583    0.8583    0.8583       120\n",
      "          sweet potato     0.9364    0.8583    0.8957       120\n",
      "              zucchini     0.9417    0.9417    0.9417       120\n",
      "\n",
      "              accuracy                         0.8915      7514\n",
      "             macro avg     0.8914    0.8897    0.8896      7514\n",
      "          weighted avg     0.8929    0.8915    0.8914      7514\n",
      "\n",
      "âœ… 'classification_report.txt' has been logged to MLflow.\n",
      "âœ… 'confusion_matrix.png' has been logged to MLflow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/19 16:32:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/19 16:32:52 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 'sorted_correct_counts.png' has been logged to MLflow.\n",
      "==================== Full Evaluation Logging Complete ====================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/19 16:33:23 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ Run 2e5d33ac2f4d496db58c9e79ff17c965 å®Œæˆã€‚æœ€ä½³é©—è­‰æº–ç¢ºç‡: 0.8925\n",
      "ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! Model: MobileNetV3-Large, Run ID: 2e5d33ac2f4d496db58c9e79ff17c965, Accuracy: 0.8925\n",
      "ğŸ’¾ æœ€ä½³æ¨¡å‹å·²æ›´æ–°ä¸¦å„²å­˜ç‚º '1best_tuned_model.h5'\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ Run 2/2: å˜—è©¦åƒæ•¸çµ„åˆ: {'model_name': 'EfficientNet-B0', 'learning_rate': 0.001, 'dropout_rate': 0.3, 'dense_units': 256, 'trainable_layers': 80}\n",
      "INFO: Using EfficientNet preprocess_input.\n",
      "Found 26307 images belonging to 60 classes.\n",
      "Found 7514 images belonging to 60 classes.\n",
      "\n",
      "-------------------- STAGE 1: Feature Extraction --------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch in layer #1 (named stem_conv)for weight stem_conv/kernel. Weight expects shape (3, 3, 1, 32). Received saved weight with shape (3, 3, 3, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# --- å•Ÿå‹•è‡ªå‹•è¨“ç·´èˆ‡èª¿åƒ ---\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# å‡½å¼å…§éƒ¨æœƒè™•ç†æ•¸æ“šç”Ÿæˆå™¨ã€é¡åˆ¥æ•¸é‡å’Œæ¬Šé‡è¨ˆç®—\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mtrain_and_tune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidation_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_accuracy\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.98\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mtrain_and_tune\u001b[39m\u001b[34m(train_path, validation_path, target_accuracy)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# STAGE 1: è¨“ç·´åˆ†é¡é ­\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m20\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m STAGE 1: Feature Extraction \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m20\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m model = \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_base_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m history_stage1 = model.fit(\n\u001b[32m    116\u001b[39m     train_gen, \n\u001b[32m    117\u001b[39m     epochs=STAGE_1_EPOCHS, \n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m     verbose=\u001b[32m1\u001b[39m\n\u001b[32m    122\u001b[39m     )\n\u001b[32m    124\u001b[39m model.save_weights(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcheckpoints/stage1/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mstage1.weights.h5\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mbuild_model\u001b[39m\u001b[34m(model_name, learning_rate, dropout_rate, dense_units, trainable_layers, num_classes, freeze_base_model)\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Base model loaded: MobileNetV3-Large\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_name == \u001b[33m'\u001b[39m\u001b[33mEfficientNet-B0\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     base_model = \u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapplications\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEfficientNetB0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimagenet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# é—œéµï¼šå°‡ base_model çš„è¼¸å…¥èˆ‡æˆ‘å€‘å®šç¾©çš„ `inputs` å¼µé‡é€£æ¥èµ·ä¾†\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Base model loaded: EfficientNetB0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cream\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\applications\\efficientnet.py:571\u001b[39m, in \u001b[36mEfficientNetB0\u001b[39m\u001b[34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, name)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;129m@keras_export\u001b[39m(\n\u001b[32m    556\u001b[39m     [\n\u001b[32m    557\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mkeras.applications.efficientnet.EfficientNetB0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    569\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mefficientnetb0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    570\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEfficientNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_top\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpooling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpooling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclassifier_activation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclassifier_activation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mb0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cream\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\applications\\efficientnet.py:434\u001b[39m, in \u001b[36mEfficientNet\u001b[39m\u001b[34m(width_coefficient, depth_coefficient, default_size, dropout_rate, drop_connect_rate, depth_divisor, activation, blocks_args, name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, weights_name)\u001b[39m\n\u001b[32m    427\u001b[39m     file_name = name + file_suffix\n\u001b[32m    428\u001b[39m     weights_path = file_utils.get_file(\n\u001b[32m    429\u001b[39m         file_name,\n\u001b[32m    430\u001b[39m         BASE_WEIGHTS_PATH + file_name,\n\u001b[32m    431\u001b[39m         cache_subdir=\u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    432\u001b[39m         file_hash=file_hash,\n\u001b[32m    433\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    436\u001b[39m     model.load_weights(weights)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cream\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\cream\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:447\u001b[39m, in \u001b[36m_set_weights\u001b[39m\u001b[34m(instance, symbolic_weights, weight_values, name, skip_mismatch)\u001b[39m\n\u001b[32m    437\u001b[39m             warnings.warn(\n\u001b[32m    438\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSkipping loading weights for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    439\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdue to mismatch in shape for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m                 stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    445\u001b[39m             )\n\u001b[32m    446\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    448\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape mismatch in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    449\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor weight \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbolic_weights[i].path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWeight expects shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mReceived saved weight \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    452\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreceived_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    453\u001b[39m         )\n\u001b[32m    454\u001b[39m     symbolic_weights[i].assign(weight_value)\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(instance, \u001b[33m\"\u001b[39m\u001b[33mfinalize_state\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m symbolic_weights:\n",
      "\u001b[31mValueError\u001b[39m: Shape mismatch in layer #1 (named stem_conv)for weight stem_conv/kernel. Weight expects shape (3, 3, 1, 32). Received saved weight with shape (3, 3, 3, 32)"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 3. åŸ·è¡Œä¸»æµç¨‹ (å·²æ›´æ–°)\n",
    "\n",
    "# --- å®šç¾©è·¯å¾‘ ---\n",
    "train_path = \"dataset_full_en_aug2/train\"\n",
    "validation_path = \"dataset_full_en_aug2/validation\"\n",
    "\n",
    "# --- å•Ÿå‹•è‡ªå‹•è¨“ç·´èˆ‡èª¿åƒ ---\n",
    "if __name__ == '__main__':\n",
    "    # å‡½å¼å…§éƒ¨æœƒè™•ç†æ•¸æ“šç”Ÿæˆå™¨ã€é¡åˆ¥æ•¸é‡å’Œæ¬Šé‡è¨ˆç®—\n",
    "    train_and_tune(\n",
    "        train_path=train_path,\n",
    "        validation_path=validation_path,\n",
    "        target_accuracy=0.98\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef659344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
