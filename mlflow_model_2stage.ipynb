{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4010c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "from PIL import Image \n",
    "from pathlib import Path\n",
    "import visualkeras\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers, layers, models\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint \n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c087ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 1. å»ºç«‹æ¨¡åž‹çš„å‡½å¼\n",
    "def build_model(learning_rate, dropout_rate, dense_units, trainable_layers, num_classes):\n",
    "    \"\"\"\n",
    "    æ ¹æ“šå‚³å…¥çš„è¶…åƒæ•¸å»ºç«‹ MobileNetV2 æ¨¡åž‹ã€‚\n",
    "\n",
    "    åƒæ•¸:\n",
    "    - learning_rate (float): å­¸ç¿’çŽ‡\n",
    "    - dropout_rate (float): Dropout æ¯”çŽ‡\n",
    "    - dense_units (int): å…¨é€£æŽ¥å±¤çš„ç¥žç¶“å…ƒæ•¸é‡\n",
    "    - trainable_layers (int): MobileNetV2 ä¸­å¯è¨“ç·´çš„å±¤æ•¸ (å¾žå¾Œå¾€å‰ç®—)\n",
    "    - num_classes (int): åˆ†é¡žçš„é¡žåˆ¥ç¸½æ•¸\n",
    "    \n",
    "    è¿”å›ž:\n",
    "    - model: å·²ç·¨è­¯çš„ Keras æ¨¡åž‹\n",
    "    \"\"\"\n",
    "    # å»ºç«‹åŸºç¤Žæ¨¡åž‹ MobileNetV2\n",
    "    base_model = keras.applications.MobileNetV2(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # è¨­å®šå¯è¨“ç·´çš„å±¤æ•¸\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-trainable_layers]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # å»ºç«‹åºåˆ—æ¨¡åž‹\n",
    "    model = keras.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(dense_units, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(dense_units, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # ç·¨è­¯æ¨¡åž‹\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6a0960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 1. MLflow è¦–è¦ºåŒ–èˆ‡æ—¥èªŒè¨˜éŒ„è¼”åŠ©å‡½å¼\n",
    "# ===================================================================\n",
    "\n",
    "def plot_and_log_history(history, filename=\"history_plots.png\"):\n",
    "    \"\"\"\n",
    "    ç¹ªè£½æº–ç¢ºçŽ‡å’Œæå¤±å‡½æ•¸çš„æ­·å²æ›²ç·šï¼Œä¸¦å°‡å…¶å„²å­˜ç‚ºåœ–ç‰‡ï¼Œæœ€å¾Œè¨˜éŒ„åˆ° MLflowã€‚\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # æº–ç¢ºçŽ‡åœ–\n",
    "    ax1.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.set_title('Accuracy over epochs')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # æå¤±å‡½æ•¸åœ–\n",
    "    ax2.plot(history.history['loss'], label='Train Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Val Loss')\n",
    "    ax2.set_title('Loss over epochs')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # å„²å­˜åœ–ç‰‡ä¸¦é—œé–‰ç¹ªåœ–\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)  # ç¢ºä¿é—œé–‰æ­£ç¢ºçš„ figure\n",
    "    \n",
    "    # å°‡åœ–ç‰‡è¨˜éŒ„åˆ° MLflow\n",
    "    mlflow.log_artifact(filename)\n",
    "    print(f\"âœ… '{filename}' has been logged to MLflow.\")\n",
    "\n",
    "def log_confusion_matrix(y_true, y_pred, class_labels, filename=\"confusion_matrix.png\"):\n",
    "    \"\"\" ç¹ªè£½ã€å„²å­˜ä¸¦è¨˜éŒ„æ¨™æº–æ··æ·†çŸ©é™£ \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(18, 14)) # å¢žåŠ åœ–ç‰‡å¤§å°ä»¥å®¹ç´æ›´å¤šé¡žåˆ¥\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels, cmap='Blues')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(filename)\n",
    "    print(f\"âœ… '{filename}' has been logged to MLflow.\")\n",
    "\n",
    "def plot_and_log_sorted_confusion_bar(y_true, y_pred, class_labels, filename=\"sorted_correct_counts.png\"):\n",
    "    \"\"\" è¨ˆç®—ã€ç¹ªè£½ä¸¦è¨˜éŒ„ä¸€å€‹é•·æ¢åœ–ï¼Œé¡¯ç¤ºæ¯å€‹é¡žåˆ¥çš„æ­£ç¢ºé æ¸¬æ•¸é‡ï¼ˆç”±é«˜åˆ°ä½ŽæŽ’åºï¼‰ã€‚ \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    correct_counts = np.diag(cm)\n",
    "    \n",
    "    sorted_idx = np.argsort(correct_counts)[::-1]\n",
    "    sorted_counts = correct_counts[sorted_idx]\n",
    "    sorted_labels = np.array(class_labels)[sorted_idx]\n",
    "\n",
    "    plt.figure(figsize=(18, 8)) # å¢žåŠ åœ–ç‰‡å¯¬åº¦\n",
    "    plt.bar(range(len(sorted_counts)), sorted_counts, color='lightgreen')\n",
    "    plt.xticks(range(len(sorted_labels)), sorted_labels, rotation=90)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Correctly Predicted Count')\n",
    "    plt.title('Correctly Predicted Count per Class (sorted)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "    mlflow.log_artifact(filename)\n",
    "    print(f\"âœ… '{filename}' has been logged to MLflow.\")\n",
    "\n",
    "def evaluate_and_log_all_reports(model, data_generator, class_labels):\n",
    "    \"\"\"\n",
    "    è©•ä¼°æ¨¡åž‹ä¸¦è¨˜éŒ„åˆ†é¡žå ±å‘Šã€æ··æ·†çŸ©é™£åŠå…¶ä»–åˆ†æžåœ–åˆ° MLflowã€‚\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*20 + \" Starting Full Evaluation for Logging \" + \"=\"*20)\n",
    "\n",
    "    # é—œéµï¼šé‡ç½® generator ä»¥ç¢ºä¿é †åºæ­£ç¢º\n",
    "    data_generator.reset()\n",
    "\n",
    "    # 1. æ¨¡åž‹é æ¸¬\n",
    "    # ä½¿ç”¨ predict() æ–¹æ³•ï¼Œä¸¦æŒ‡å®š stepsï¼Œå¯ä»¥ç¢ºä¿è™•ç†å®Œæ‰€æœ‰æ¨£æœ¬\n",
    "    predictions = model.predict(data_generator, steps=len(data_generator), verbose=1)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # 2. å–å¾—çœŸå¯¦æ¨™ç±¤\n",
    "    # æ³¨æ„ï¼šgenerator çš„ classes å±¬æ€§åŒ…å«äº†æ‰€æœ‰æ¨£æœ¬çš„çœŸå¯¦æ¨™ç±¤\n",
    "    y_true = data_generator.classes\n",
    "\n",
    "    # ç¢ºä¿é•·åº¦ä¸€è‡´ï¼Œé€™åœ¨ generator batch_size ç„¡æ³•æ•´é™¤ç¸½æ¨£æœ¬æ•¸æ™‚å¾ˆé‡è¦\n",
    "    if len(y_pred) != len(y_true):\n",
    "        print(f\"âš ï¸ y_pred length {len(y_pred)} vs y_true length {len(y_true)}. This is normal if batch_size doesn't divide total samples.\")\n",
    "        # `predict` æœƒè™•ç†å®Œæ‰€æœ‰æ¨£æœ¬ï¼Œæ‰€ä»¥ y_true ä¹Ÿæ‡‰è©²æ˜¯å®Œæ•´çš„\n",
    "        y_true = y_true[:len(y_pred)]\n",
    "\n",
    "    # 3. ç”¢ç”Ÿä¸¦è¨˜éŒ„åˆ†é¡žå ±å‘Š\n",
    "    report = classification_report(y_true, y_pred, target_names=class_labels, digits=4, zero_division=0)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "    mlflow.log_text(report, \"classification_report.txt\")\n",
    "    print(\"âœ… 'classification_report.txt' has been logged to MLflow.\")\n",
    "\n",
    "    # 4. è¨˜éŒ„æ··æ·†çŸ©é™£\n",
    "    log_confusion_matrix(y_true, y_pred, class_labels)\n",
    "\n",
    "    # 5. è¨˜éŒ„æŽ’åºå¾Œçš„æ­£ç¢ºé æ¸¬æ•¸é‡é•·æ¢åœ–\n",
    "    plot_and_log_sorted_confusion_bar(y_true, y_pred, class_labels)\n",
    "    \n",
    "    print(\"=\"*20 + \" Full Evaluation Logging Complete \" + \"=\"*20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb2b42",
   "metadata": {},
   "source": [
    "### å–®éšŽæ®µè¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21242e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 2. è‡ªå‹•è¨“ç·´èˆ‡èª¿åƒçš„ä¸»å‡½å¼ (å·²æ›´æ–°)\n",
    "import itertools\n",
    "\n",
    "def train_and_tune(train_gen, val_gen, class_labels, class_weight, num_classes, target_accuracy=0.9):\n",
    "    \"\"\"\n",
    "    è‡ªå‹•è¨“ç·´å’Œèª¿æ•´è¶…åƒæ•¸ï¼Œç›´åˆ°é©—è­‰æº–ç¢ºçŽ‡é”åˆ°ç›®æ¨™ã€‚\n",
    "    ä½¿ç”¨ MLflow è‡ªå‹•è¨˜éŒ„åƒæ•¸ã€æŒ‡æ¨™ã€åœ–è¡¨å’Œæ¨¡åž‹ã€‚\n",
    "    \"\"\"\n",
    "    # å®šç¾©è¶…åƒæ•¸çš„æœå°‹ç©ºé–“\n",
    "    param_space = {\n",
    "        'learning_rate': [0.0001],\n",
    "        'dropout_rate': [0.5],\n",
    "        'dense_units': [256],\n",
    "        'trainable_layers': [60] # å¾ž MobileNetV2 çš„å¾Œé¢é–‹å§‹è¨“ç·´çš„å±¤æ•¸\n",
    "    }\n",
    "    \n",
    "    keys, values = zip(*param_space.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    best_model = None\n",
    "    best_run_id = None\n",
    "\n",
    "    mlflow.set_experiment(\"MobileNetV2 mixup è³‡æ–™é›†3è¨“ç·´\")\n",
    "    \n",
    "    for i, params in enumerate(param_combinations):\n",
    "        if best_val_accuracy >= target_accuracy:\n",
    "            print(f\"ðŸŽ¯ ç›®æ¨™å·²é”æˆ (Accuracy >= {target_accuracy:.2f})ã€‚åœæ­¢æœå°‹ã€‚\")\n",
    "            break\n",
    "\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"ðŸš€ Run {i+1}/{len(param_combinations)}: å˜—è©¦åƒæ•¸çµ„åˆ: {params}\")\n",
    "\n",
    "        with mlflow.start_run() as run:\n",
    "            # è¨˜éŒ„è¶…åƒæ•¸\n",
    "            mlflow.log_params(params)\n",
    "            \n",
    "            # å»ºç«‹ä¸¦ç·¨è­¯æ¨¡åž‹\n",
    "            model = build_model(num_classes=num_classes, **params)\n",
    "            \n",
    "            # è¨­å®šå›žå‘¼å‡½å¼\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-8)\n",
    "            checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model_checkpoint.keras\", monitor=\"val_loss\", save_best_only=True)\n",
    "            \n",
    "            # æ¨¡åž‹è¨“ç·´\n",
    "            history = model.fit(\n",
    "                train_gen,\n",
    "                epochs=30,\n",
    "                validation_data=val_gen,\n",
    "                class_weight=class_weight,\n",
    "                callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # === MLflow è¨˜éŒ„å€ ===\n",
    "            \n",
    "            # 1. è¨˜éŒ„ä¸»è¦æŒ‡æ¨™\n",
    "            val_accuracy = max(history.history['val_accuracy'])\n",
    "            mlflow.log_metric(\"best_val_accuracy\", val_accuracy)\n",
    "            mlflow.log_metric(\"final_train_accuracy\", history.history['accuracy'][-1])\n",
    "            mlflow.log_metric(\"stopped_epoch\", len(history.history['val_accuracy']))\n",
    "\n",
    "            # 2. è¨˜éŒ„è¨“ç·´æ­·å²åœ–è¡¨\n",
    "            plot_and_log_history(history)\n",
    "            \n",
    "            # 3. è¨˜éŒ„å®Œæ•´çš„è©•ä¼°å ±å‘Š (æ··æ·†çŸ©é™£, åˆ†é¡žå ±å‘Šç­‰)\n",
    "            # ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡åž‹ (EarlyStopping æœƒè‡ªå‹•é‚„åŽŸæœ€ä½³æ¬Šé‡)\n",
    "            evaluate_and_log_all_reports(model, val_gen, class_labels)\n",
    "            \n",
    "            # 4. è¨˜éŒ„æ¨¡åž‹\n",
    "            mlflow.keras.log_model(model, \"model\", signature=None) # signature=False å¯ä»¥åŠ å¿«å„²å­˜é€Ÿåº¦\n",
    "            \n",
    "            print(f\"âœ”ï¸ Run {run.info.run_id} å®Œæˆã€‚æœ€ä½³é©—è­‰æº–ç¢ºçŽ‡: {val_accuracy:.4f}\")\n",
    "            \n",
    "            # æ›´æ–°å…¨å±€æœ€ä½³æ¨¡åž‹\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_model = model\n",
    "                best_run_id = run.info.run_id\n",
    "                print(f\"ðŸŽ‰ æ–°çš„æœ€ä½³æ¨¡åž‹! Run ID: {best_run_id}, Accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ðŸ è‡ªå‹•èª¿åƒå®Œæˆã€‚\")\n",
    "    if best_model:\n",
    "        print(f\"ðŸ† æœ€çµ‚æœ€ä½³æ¨¡åž‹çš„ Run ID ç‚º: {best_run_id}\")\n",
    "        print(f\"ðŸ† æœ€çµ‚æœ€ä½³é©—è­‰æº–ç¢ºçŽ‡ç‚º: {best_val_accuracy:.4f}\")\n",
    "        best_model.save('best_tuned_model.h5')\n",
    "        print(\"ðŸ’¾ æœ€ä½³æ¨¡åž‹å·²å„²å­˜ç‚º 'best_tuned_model.h5'\")\n",
    "    else:\n",
    "        print(\"âŒ æœªèƒ½æˆåŠŸè¨“ç·´ä»»ä½•æ¨¡åž‹ã€‚\")\n",
    "        \n",
    "    return best_model, best_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4eebf",
   "metadata": {},
   "source": [
    "### 2éšŽæ®µè¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0898cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 2. è‡ªå‹•è¨“ç·´èˆ‡èª¿åƒçš„ä¸»å‡½å¼ (å…¨é¢å„ªåŒ–ç‰ˆ)\n",
    "def train_and_tune(train_gen, val_gen, class_labels, class_weight, num_classes, target_accuracy=0.9):\n",
    "    \"\"\"\n",
    "    è‡ªå‹•åŸ·è¡Œå…©éšŽæ®µè¨“ç·´å’Œè¶…åƒæ•¸èª¿æ•´ã€‚\n",
    "    å¯¦ä½œäº†å¤šé …é·ç§»å­¸ç¿’æœ€ä½³å¯¦è¸ã€‚\n",
    "    \"\"\"\n",
    "    param_space = {\n",
    "        'learning_rate': [0.0001],\n",
    "        'dropout_rate': [0.5],\n",
    "        'dense_units': [256],\n",
    "        'trainable_layers': [60] \n",
    "    }\n",
    "    \n",
    "    STAGE_1_EPOCHS = 30\n",
    "    STAGE_2_EPOCHS = 50\n",
    "    FINE_TUNE_LR_MULTIPLIER = 0.1\n",
    "    \n",
    "    keys, values = zip(*param_space.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    best_run_id = None\n",
    "\n",
    "    mlflow.set_experiment(\"MobileNetV2æ–°2éšŽæ®µ 48ç¨® è³‡æ–™å¤¾3\")\n",
    "    \n",
    "    for i, params in enumerate(param_combinations):\n",
    "\n",
    "        if best_val_accuracy >= target_accuracy:\n",
    "            print(f\"ðŸŽ¯ ç›®æ¨™å·²é”æˆ (Accuracy >= {target_accuracy:.2f})ã€‚åœæ­¢æœå°‹ã€‚\")\n",
    "            break\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"ðŸš€ Run {i+1}/{len(param_combinations)}: å˜—è©¦åƒæ•¸çµ„åˆ: {params}\")\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"_run_{i+1}\") as run:\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            # STAGE 1: Feature Extraction\n",
    "            print(\"\\n\" + \"-\" * 20 + \" STAGE 1: Feature Extraction \" + \"-\" * 20)\n",
    "            \n",
    "            # ### å„ªåŒ– 1ï¼šæŽ¥æ”¶ model å’Œ base_model ###\n",
    "            model, base_model = build_model(\n",
    "                **params, num_classes=num_classes, freeze_base_model=True\n",
    "            )\n",
    "            \n",
    "            history_stage1 = model.fit(\n",
    "                train_gen, epochs=STAGE_1_EPOCHS, validation_data=val_gen,\n",
    "                callbacks=[EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)],\n",
    "                class_weight=class_weight, verbose=1\n",
    "            )\n",
    "            \n",
    "            # STAGE 2: Fine-Tuning\n",
    "            print(\"\\n\" + \"-\" * 20 + \" STAGE 2: Fine-Tuning \" + \"-\" * 20)\n",
    "\n",
    "            # --- ç›´æŽ¥åœ¨ç¾æœ‰ model ç‰©ä»¶ä¸Šæ“ä½œ ---\n",
    "            base_model.trainable = True\n",
    "            \n",
    "            trainable_layers_count = params['trainable_layers']\n",
    "            if trainable_layers_count > 0 and trainable_layers_count < len(base_model.layers):\n",
    "                print(f\"ðŸ”¥ Fine-tuning: Unfreezing the last {trainable_layers_count} layers.\")\n",
    "                for layer in base_model.layers[:-trainable_layers_count]:\n",
    "                    layer.trainable = False\n",
    "            else:\n",
    "                print(\"ðŸ”¥ Fine-tuning: Unfreezing all base model layers.\")\n",
    "            \n",
    "            # ### å„ªåŒ– 3aï¼šåœ¨å¾®èª¿æ™‚å‡çµ BatchNormalization å±¤ ###\n",
    "            for layer in base_model.layers:\n",
    "                if isinstance(layer, layers.BatchNormalization):\n",
    "                    layer.trainable = False\n",
    "            print(\"ðŸ§Š All BatchNormalization layers in the base model have been frozen.\")\n",
    "\n",
    "            # ç”¨æ›´ä½Žçš„å­¸ç¿’çŽ‡é‡æ–°ç·¨è­¯æ¨¡åž‹\n",
    "            fine_tune_lr = params['learning_rate'] * FINE_TUNE_LR_MULTIPLIER\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "                loss=\"categorical_crossentropy\",\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            callbacks_stage2 = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-9),\n",
    "                ModelCheckpoint(\"model_checkpoint.keras\", monitor=\"val_loss\", save_best_only=True)\n",
    "            ]\n",
    "            history_stage2 = model.fit(\n",
    "                train_gen, epochs=STAGE_2_EPOCHS, validation_data=val_gen,\n",
    "                class_weight=class_weight, callbacks=callbacks_stage2, verbose=1\n",
    "            )\n",
    "            \n",
    "            # --- MLflow è¨˜éŒ„ ---\n",
    "            full_history = {}\n",
    "            for key in history_stage1.history.keys():\n",
    "                full_history[key] = history_stage1.history[key] + history_stage2.history[key]\n",
    "            \n",
    "            # ### å„ªåŒ– 3bï¼šè¨ˆç®—æ•´å€‹è¨“ç·´éŽç¨‹ä¸­çš„æœ€ä½³é©—è­‰æº–ç¢ºçŽ‡ ###\n",
    "            # ç¢ºä¿ history å­—å…¸ä¸æ˜¯ç©ºçš„\n",
    "            best_s1_acc = max(history_stage1.history.get('val_accuracy', [0]))\n",
    "            best_s2_acc = max(history_stage2.history.get('val_accuracy', [0]))\n",
    "            current_run_best_val_accuracy = max(best_s1_acc, best_s2_acc)\n",
    "            \n",
    "            mlflow.log_metric(\"best_val_accuracy\", current_run_best_val_accuracy)\n",
    "            \n",
    "            plot_and_log_history(type('History', (), {'history': full_history})())\n",
    "            evaluate_and_log_all_reports(model, val_gen, class_labels)\n",
    "            mlflow.keras.log_model(model, \"model\")\n",
    "            \n",
    "            print(f\"âœ”ï¸ Run {run.info.run_id} å®Œæˆã€‚æœ¬è¼ªæœ€ä½³é©—è­‰æº–ç¢ºçŽ‡: {current_run_best_val_accuracy:.4f}\")\n",
    "            \n",
    "            if current_run_best_val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = current_run_best_val_accuracy\n",
    "                best_run_id = run.info.run_id\n",
    "                print(f\"ðŸŽ‰ æ–°çš„æœ€ä½³æ¨¡åž‹! Model: {params['model_name']}, Run ID: {best_run_id}, Accuracy: {best_val_accuracy:.4f}\")\n",
    "                model.save(f'checkpoints/bestmodel/{i+1}{params['model_name']}_{best_val_accuracy:.4f}.keras')\n",
    "                print(f\"ðŸ’¾ æœ€ä½³æ¨¡åž‹å·²æ›´æ–°ä¸¦å„²å­˜ç‚º '{i+1}{params['model_name']}_{best_val_accuracy:.4f}.keras'\")\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ðŸ è‡ªå‹•èª¿åƒå®Œæˆã€‚\")\n",
    "    if best_run_id:\n",
    "        print(f\"ðŸ† æœ€çµ‚æœ€ä½³æ¨¡åž‹çš„ Run ID ç‚º: {best_run_id}\")\n",
    "        print(f\"ðŸ† æœ€çµ‚æœ€ä½³é©—è­‰æº–ç¢ºçŽ‡ç‚º: {best_val_accuracy:.4f}\")\n",
    "    else:\n",
    "        print(\"âŒ æœªèƒ½æˆåŠŸè¨“ç·´ä»»ä½•æ¨¡åž‹ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09adec77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21859 images belonging to 50 classes.\n",
      "Found 6241 images belonging to 50 classes.\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ Run 1/1: å˜—è©¦åƒæ•¸çµ„åˆ: {'learning_rate': 0.0001, 'dropout_rate': 0.5, 'dense_units': 256, 'trainable_layers': 60}\n",
      "\n",
      "-------------------- STAGE 1: Feature Extraction --------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "build_model() got an unexpected keyword argument 'freeze_base_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# --- å•Ÿå‹•è‡ªå‹•è¨“ç·´èˆ‡èª¿åƒ ---\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# å°‡æ‰€æœ‰éœ€è¦çš„åƒæ•¸å‚³å…¥\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     best_model, best_accuracy = \u001b[43mtrain_and_tune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_generator_aug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_gen\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_accuracy\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.99\u001b[39;49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mtrain_and_tune\u001b[39m\u001b[34m(train_gen, val_gen, class_labels, class_weight, num_classes, target_accuracy)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m20\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m STAGE 1: Feature Extraction \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m20\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# ### å„ªåŒ– 1ï¼šæŽ¥æ”¶ model å’Œ base_model ###\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m model, base_model = \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze_base_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     45\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m history_stage1 = model.fit(\n\u001b[32m     48\u001b[39m     train_gen, epochs=STAGE_1_EPOCHS, validation_data=val_gen,\n\u001b[32m     49\u001b[39m     callbacks=[EarlyStopping(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m8\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m)],\n\u001b[32m     50\u001b[39m     class_weight=class_weight, verbose=\u001b[32m1\u001b[39m\n\u001b[32m     51\u001b[39m )\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# STAGE 2: Fine-Tuning\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: build_model() got an unexpected keyword argument 'freeze_base_model'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 3. åŸ·è¡Œä¸»æµç¨‹ (å·²æ›´æ–°)\n",
    "\n",
    "# --- è³‡æ–™æº–å‚™ ---\n",
    "train_path = \"dataset_full_en_aug3/train\"\n",
    "validation_path = \"dataset_full_en_aug3/validation\"\n",
    "num_classes = len(os.listdir(train_path))\n",
    "\n",
    "train_datagen_aug = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.9, 1.1],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator_aug = train_datagen_aug.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True # è¨“ç·´é›†éœ€è¦ shuffle\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False # é©—è­‰/æ¸¬è©¦é›†çµ•ä¸èƒ½ shuffleï¼Œä»¥ç¢ºä¿æ¨™ç±¤é †åºæ­£ç¢º\n",
    ")\n",
    "\n",
    "\n",
    "# å¾ž generator ä¸­ç²å–é¡žåˆ¥åç¨±å’Œç´¢å¼•çš„å°æ‡‰é—œä¿‚\n",
    "class_labels = list(train_generator_aug.class_indices.keys())\n",
    "\n",
    "# è¨ˆç®— class_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_indices = np.unique(train_generator_aug.classes)\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=class_indices,\n",
    "    y=train_generator_aug.classes\n",
    ")\n",
    "class_weight_dict = dict(zip(class_indices, class_weights_array))\n",
    "\n",
    "\n",
    "# --- å•Ÿå‹•è‡ªå‹•è¨“ç·´èˆ‡èª¿åƒ ---\n",
    "if __name__ == '__main__':\n",
    "    # å°‡æ‰€æœ‰éœ€è¦çš„åƒæ•¸å‚³å…¥\n",
    "    best_model, best_accuracy = train_and_tune(\n",
    "        train_gen=train_generator_aug,\n",
    "        val_gen=validation_generator,\n",
    "        class_labels=class_labels,\n",
    "        class_weight=class_weight_dict,\n",
    "        num_classes=num_classes,\n",
    "        target_accuracy=0.99\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1f26924",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbest_model\u001b[49m.save(\u001b[33m'\u001b[39m\u001b[33mmodel_mnV2(best).keras\u001b[39m\u001b[33m'\u001b[39m) \n",
      "\u001b[31mNameError\u001b[39m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "best_model.save('model_mnV2(best).keras') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
