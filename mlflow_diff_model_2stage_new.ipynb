{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d8ad028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "from PIL import Image \n",
    "from pathlib import Path\n",
    "import visualkeras\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from tensorflow.keras.applications import mobilenet_v3, efficientnet\n",
    "import itertools # ç¢ºä¿ itertools å·²åŒ¯å…¥\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import keras_cv\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers, layers, models\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint \n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70b5b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 1. å»ºç«‹æ¨¡å‹çš„å‡½å¼ (æœ€çµ‚ç©©å¥ç‰ˆï¼šç¹é Keras 3 çš„ input_tensor Bug)\n",
    "def build_model(model_name, learning_rate, dropout_rate, dense_units, trainable_layers, num_classes, \n",
    "                freeze_base_model=False, loss_function='categorical_crossentropy'):\n",
    "    \"\"\"\n",
    "    æ ¹æ“šå‚³å…¥çš„è¶…åƒæ•¸å»ºç«‹ä¸¦ç·¨è­¯æŒ‡å®šçš„ Keras æ¨¡å‹ã€‚\n",
    "    *** ä¿®æ­£ï¼šæ”¹è®Šæ¨¡å‹æ§‹å»ºæ–¹å¼ï¼Œä»¥é¿é–‹ Keras 3 åœ¨ä½¿ç”¨ input_tensor æ™‚çš„ shape inference bug ***\n",
    "    \"\"\"\n",
    "    input_shape = (224, 224, 3) \n",
    "    \n",
    "    # --- æ­¥é©Ÿ 1ï¼šåƒä¹‹å‰ä¸€æ¨£ï¼Œå®šç¾©è¼¸å…¥å’Œæ•¸æ“šå¢å¼·å±¤ ---\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    \n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(0.15),\n",
    "            layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "            layers.RandomErasing(scale=(0.02, 0.1), value_range=[0, 255]),\n",
    "        ],\n",
    "        name=\"data_augmentation\",\n",
    "    )\n",
    "\n",
    "    # --- æ­¥é©Ÿ 2ï¼šé—œéµä¿®æ”¹ - å…ˆç¨ç«‹å‰µå»ºåŸºç¤æ¨¡å‹ï¼Œä¸å‚³å…¥ input_tensor ---\n",
    "    if model_name == 'MobileNetV3-Large':\n",
    "        # æ³¨æ„ï¼šæˆ‘å€‘ä¸å†ä½¿ç”¨ input_tensor åƒæ•¸\n",
    "        base_model = keras.applications.MobileNetV3Large(\n",
    "            input_shape=input_shape, include_top=False, weights='imagenet'\n",
    "        )\n",
    "    elif model_name == 'EfficientNet-B0':\n",
    "        # æ³¨æ„ï¼šæˆ‘å€‘ä¸å†ä½¿ç”¨ input_tensor åƒæ•¸\n",
    "        base_model = keras.applications.EfficientNetB0(\n",
    "            input_shape=input_shape, include_top=False, weights='imagenet'\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"ä¸æ”¯æ´çš„æ¨¡å‹åç¨±: {model_name}ã€‚\")\n",
    "        \n",
    "    # --- æ­¥é©Ÿ 3ï¼šè¨­å®šåŸºç¤æ¨¡å‹çš„å¯è¨“ç·´æ€§ ---\n",
    "    if freeze_base_model:\n",
    "        base_model.trainable = False\n",
    "    else:\n",
    "        base_model.trainable = True\n",
    "\n",
    "    # --- æ­¥é©Ÿ 4ï¼šæ‰‹å‹•é€£æ¥è¨ˆç®—åœ– (Graph) ---\n",
    "    # 1. åŸå§‹è¼¸å…¥ -> æ•¸æ“šå¢å¼·\n",
    "    x = data_augmentation(inputs)\n",
    "    \n",
    "    # 2. æ•¸æ“šå¢å¼·çš„è¼¸å‡º -> åŸºç¤æ¨¡å‹\n",
    "    #    åœ¨å‡çµéšæ®µï¼ŒBNå±¤æ‡‰åœ¨æ¨æ–·æ¨¡å¼ä¸‹é‹è¡Œï¼Œæ‰€ä»¥ training=False\n",
    "    x = base_model(x, training=False) \n",
    "    \n",
    "    # 3. åŸºç¤æ¨¡å‹çš„è¼¸å‡º -> å…¨å±€æ± åŒ–å’Œåˆ†é¡é ­\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # --- æ­¥é©Ÿ 5ï¼šå‰µå»ºæœ€çµ‚æ¨¡å‹ ---\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # --- æ­¥é©Ÿ 6ï¼šç·¨è­¯æ¨¡å‹ ---\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=keras_cv.losses.FocalLoss(from_logits=False, label_smoothing=0.1), # 2. ä½¿ç”¨ Keras-CV çš„ FocalLoss\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b38f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MLflow è¦–è¦ºåŒ–èˆ‡æ—¥èªŒè¨˜éŒ„è¼”åŠ©å‡½å¼ (ç¶­æŒä¸è®Š)\n",
    "\n",
    "def plot_and_log_history(history, filename=\"history_plots.png\"):\n",
    "    \"\"\"\n",
    "    ç¹ªè£½æº–ç¢ºç‡å’Œæå¤±å‡½æ•¸çš„æ­·å²æ›²ç·šï¼Œä¸¦å°‡å…¶å„²å­˜ç‚ºåœ–ç‰‡ï¼Œæœ€å¾Œè¨˜éŒ„åˆ° MLflowã€‚\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # æº–ç¢ºç‡åœ–\n",
    "    ax1.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.set_title('Accuracy over epochs')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # æå¤±å‡½æ•¸åœ–\n",
    "    ax2.plot(history.history['loss'], label='Train Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Val Loss')\n",
    "    ax2.set_title('Loss over epochs')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # å„²å­˜åœ–ç‰‡ä¸¦é—œé–‰ç¹ªåœ–\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)  # ç¢ºä¿é—œé–‰æ­£ç¢ºçš„ figure\n",
    "    \n",
    "    # å°‡åœ–ç‰‡è¨˜éŒ„åˆ° MLflow\n",
    "    mlflow.log_artifact(filename)\n",
    "    print(f\"âœ… '{filename}' has been logged to MLflow.\")\n",
    "\n",
    "def log_confusion_matrix(y_true, y_pred, class_labels, filename=\"confusion_matrix.png\"):\n",
    "    \"\"\" ç¹ªè£½ã€å„²å­˜ä¸¦è¨˜éŒ„æ¨™æº–æ··æ·†çŸ©é™£ \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(18, 14)) # å¢åŠ åœ–ç‰‡å¤§å°ä»¥å®¹ç´æ›´å¤šé¡åˆ¥\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels, cmap='Blues')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(filename)\n",
    "    print(f\"âœ… '{filename}' has been logged to MLflow.\")\n",
    "\n",
    "def plot_and_log_sorted_confusion_bar(y_true, y_pred, class_labels, filename=\"sorted_correct_counts.png\"):\n",
    "    \"\"\" è¨ˆç®—ã€ç¹ªè£½ä¸¦è¨˜éŒ„ä¸€å€‹é•·æ¢åœ–ï¼Œé¡¯ç¤ºæ¯å€‹é¡åˆ¥çš„æ­£ç¢ºé æ¸¬æ•¸é‡ï¼ˆç”±é«˜åˆ°ä½æ’åºï¼‰ã€‚ \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    correct_counts = np.diag(cm)\n",
    "    \n",
    "    sorted_idx = np.argsort(correct_counts)[::-1]\n",
    "    sorted_counts = correct_counts[sorted_idx]\n",
    "    sorted_labels = np.array(class_labels)[sorted_idx]\n",
    "\n",
    "    plt.figure(figsize=(18, 8)) # å¢åŠ åœ–ç‰‡å¯¬åº¦\n",
    "    plt.bar(range(len(sorted_counts)), sorted_counts, color='lightgreen')\n",
    "    plt.xticks(range(len(sorted_labels)), sorted_labels, rotation=90)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Correctly Predicted Count')\n",
    "    plt.title('Correctly Predicted Count per Class (sorted)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "    mlflow.log_artifact(filename)\n",
    "    print(f\"âœ… '{filename}' has been logged to MLflow.\")\n",
    "\n",
    "def evaluate_and_log_all_reports(model, data_generator, class_labels):\n",
    "    \"\"\"\n",
    "    è©•ä¼°æ¨¡å‹ä¸¦è¨˜éŒ„åˆ†é¡å ±å‘Šã€æ··æ·†çŸ©é™£åŠå…¶ä»–åˆ†æåœ–åˆ° MLflowã€‚\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*20 + \" Starting Full Evaluation for Logging \" + \"=\"*20)\n",
    "\n",
    "    # é—œéµï¼šé‡ç½® generator ä»¥ç¢ºä¿é †åºæ­£ç¢º\n",
    "    data_generator.reset()\n",
    "\n",
    "    # 1. æ¨¡å‹é æ¸¬\n",
    "    # ä½¿ç”¨ predict() æ–¹æ³•ï¼Œä¸¦æŒ‡å®š stepsï¼Œå¯ä»¥ç¢ºä¿è™•ç†å®Œæ‰€æœ‰æ¨£æœ¬\n",
    "    predictions = model.predict(data_generator, steps=len(data_generator), verbose=1)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # 2. å–å¾—çœŸå¯¦æ¨™ç±¤\n",
    "    # æ³¨æ„ï¼šgenerator çš„ classes å±¬æ€§åŒ…å«äº†æ‰€æœ‰æ¨£æœ¬çš„çœŸå¯¦æ¨™ç±¤\n",
    "    y_true = data_generator.classes\n",
    "\n",
    "    # ç¢ºä¿é•·åº¦ä¸€è‡´ï¼Œé€™åœ¨ generator batch_size ç„¡æ³•æ•´é™¤ç¸½æ¨£æœ¬æ•¸æ™‚å¾ˆé‡è¦\n",
    "    if len(y_pred) != len(y_true):\n",
    "        print(f\"âš ï¸ y_pred length {len(y_pred)} vs y_true length {len(y_true)}. This is normal if batch_size doesn't divide total samples.\")\n",
    "        # `predict` æœƒè™•ç†å®Œæ‰€æœ‰æ¨£æœ¬ï¼Œæ‰€ä»¥ y_true ä¹Ÿæ‡‰è©²æ˜¯å®Œæ•´çš„\n",
    "        y_true = y_true[:len(y_pred)]\n",
    "\n",
    "    # 3. ç”¢ç”Ÿä¸¦è¨˜éŒ„åˆ†é¡å ±å‘Š\n",
    "    report = classification_report(y_true, y_pred, target_names=class_labels, digits=4, zero_division=0)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "    mlflow.log_text(report, \"classification_report.txt\")\n",
    "    print(\"âœ… 'classification_report.txt' has been logged to MLflow.\")\n",
    "\n",
    "    # 4. è¨˜éŒ„æ··æ·†çŸ©é™£\n",
    "    log_confusion_matrix(y_true, y_pred, class_labels)\n",
    "\n",
    "    # 5. è¨˜éŒ„æ’åºå¾Œçš„æ­£ç¢ºé æ¸¬æ•¸é‡é•·æ¢åœ–\n",
    "    plot_and_log_sorted_confusion_bar(y_true, y_pred, class_labels)\n",
    "    \n",
    "    print(\"=\"*20 + \" Full Evaluation Logging Complete \" + \"=\"*20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ad3f6",
   "metadata": {},
   "source": [
    "### å…©ç¨®æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b30e6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 2. è‡ªå‹•è¨“ç·´èˆ‡èª¿åƒçš„ä¸»å‡½å¼ (å…¨é¢å„ªåŒ–ç‰ˆ)\n",
    "def train_and_tune(train_path, validation_path, target_accuracy):\n",
    "    \"\"\"\n",
    "    è‡ªå‹•åŸ·è¡Œå…©éšæ®µè¨“ç·´å’Œè¶…åƒæ•¸èª¿æ•´ã€‚\n",
    "    å¯¦ä½œäº†å¤šé …é·ç§»å­¸ç¿’æœ€ä½³å¯¦è¸ã€‚\n",
    "    \"\"\"\n",
    "    param_space = {\n",
    "        'model_name': [\n",
    "            'MobileNetV3-Large', \n",
    "            # 'EfficientNet-B0'\n",
    "        ],\n",
    "        'learning_rate': [0.001],\n",
    "        'dropout_rate': [0.3,0.5],\n",
    "        'dense_units': [256],\n",
    "        'trainable_layers': [80,100,120],\n",
    "\n",
    "        # 'learning_rate': [0.001],\n",
    "        # 'dropout_rate': [0.3],\n",
    "        # 'dense_units': [256],\n",
    "        # 'trainable_layers': [80]\n",
    "    }\n",
    "    \n",
    "    STAGE_1_EPOCHS = 20\n",
    "    STAGE_2_EPOCHS = 50\n",
    "    FINE_TUNE_LR_MULTIPLIER = 0.1\n",
    "    \n",
    "    keys, values = zip(*param_space.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    best_run_id = None\n",
    "\n",
    "    mlflow.set_experiment(\"MobileNetV3 & EfficientNet-B0 æ–°2éšæ®µ focal_loss+RandomErasing 60ç¨®(è³‡æ–™å¤¾6)\")\n",
    "    \n",
    "    for i, params in enumerate(param_combinations):\n",
    "\n",
    "        if best_val_accuracy >= target_accuracy:\n",
    "            print(f\"ğŸ¯ ç›®æ¨™å·²é”æˆ (Accuracy >= {target_accuracy:.2f})ã€‚åœæ­¢æœå°‹ã€‚\")\n",
    "            break\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"ğŸš€ Run {i+1}/{len(param_combinations)}: å˜—è©¦åƒæ•¸çµ„åˆ: {params}\")\n",
    "\n",
    "        if params['model_name'] == 'MobileNetV3-Large':\n",
    "            preprocessing_function = mobilenet_v3.preprocess_input\n",
    "        elif params['model_name'] == 'EfficientNet-B0':\n",
    "            preprocessing_function = efficientnet.preprocess_input\n",
    "        else:\n",
    "            raise ValueError(f\"æœªçŸ¥çš„æ¨¡å‹: {params['model_name']}\")\n",
    "\n",
    "        # train_datagen_aug = ImageDataGenerator(\n",
    "        #     preprocessing_function=preprocessing_function,\n",
    "        #     rotation_range=15,\n",
    "        #     width_shift_range=0.1,\n",
    "        #     height_shift_range=0.1,\n",
    "        #     zoom_range=0.1,\n",
    "        #     horizontal_flip=True,\n",
    "        #     # ### å„ªåŒ– 2ï¼šç‚ºé¿å…æ•¸æ“šåˆ†å¸ƒåç§»ï¼Œå»ºè­°å„ªå…ˆç§»é™¤äº®åº¦å¢å¼· ###\n",
    "        #     # brightness_range=[0.9, 1.1],\n",
    "        #     fill_mode='nearest'\n",
    "        # )\n",
    "\n",
    "        train_datagen_simple  = ImageDataGenerator(preprocessing_function=preprocessing_function)\n",
    "\n",
    "        validation_datagen = ImageDataGenerator(preprocessing_function=preprocessing_function)\n",
    "\n",
    "        train_gen = train_datagen_simple.flow_from_directory(\n",
    "            train_path, target_size=(224, 224), batch_size=32, class_mode='categorical', shuffle=True\n",
    "        )\n",
    "        val_gen = validation_datagen.flow_from_directory(\n",
    "            validation_path, target_size=(224, 224), batch_size=32, class_mode='categorical', shuffle=False\n",
    "        )\n",
    "\n",
    "        num_classes = train_gen.num_classes\n",
    "        class_labels = list(train_gen.class_indices.keys())\n",
    "\n",
    "        with open('classes_new.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for cls in class_labels:\n",
    "                writer.writerow([cls])\n",
    "        \n",
    "        class_indices = np.unique(train_gen.classes)\n",
    "        class_weights_array = compute_class_weight('balanced', classes=class_indices, y=train_gen.classes)\n",
    "        class_weight = dict(zip(class_indices, class_weights_array))\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"{params['model_name']}_run_{i+1}\") as run:\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            # STAGE 1: Feature Extraction\n",
    "            print(\"\\n\" + \"-\" * 20 + \" STAGE 1: Feature Extraction \" + \"-\" * 20)\n",
    "            \n",
    "            # ### å„ªåŒ– 1ï¼šæ¥æ”¶ model å’Œ base_model ###\n",
    "            model, base_model = build_model(\n",
    "                **params, num_classes=num_classes, freeze_base_model=True\n",
    "            )\n",
    "            \n",
    "            history_stage1 = model.fit(\n",
    "                train_gen, epochs=STAGE_1_EPOCHS, validation_data=val_gen,\n",
    "                callbacks=[EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)],\n",
    "                class_weight=class_weight, verbose=1\n",
    "            )\n",
    "            \n",
    "            # STAGE 2: Fine-Tuning\n",
    "            print(\"\\n\" + \"-\" * 20 + \" STAGE 2: Fine-Tuning \" + \"-\" * 20)\n",
    "\n",
    "            # --- ç›´æ¥åœ¨ç¾æœ‰ model ç‰©ä»¶ä¸Šæ“ä½œ ---\n",
    "            base_model.trainable = True\n",
    "            \n",
    "            trainable_layers_count = params['trainable_layers']\n",
    "            if trainable_layers_count > 0 and trainable_layers_count < len(base_model.layers):\n",
    "                print(f\"ğŸ”¥ Fine-tuning: Unfreezing the last {trainable_layers_count} layers.\")\n",
    "                for layer in base_model.layers[:-trainable_layers_count]:\n",
    "                    layer.trainable = False\n",
    "            else:\n",
    "                print(\"ğŸ”¥ Fine-tuning: Unfreezing all base model layers.\")\n",
    "            \n",
    "            # ### å„ªåŒ– 3aï¼šåœ¨å¾®èª¿æ™‚å‡çµ BatchNormalization å±¤ ###\n",
    "            for layer in base_model.layers:\n",
    "                if isinstance(layer, layers.BatchNormalization):\n",
    "                    layer.trainable = False\n",
    "            print(\"ğŸ§Š All BatchNormalization layers in the base model have been frozen.\")\n",
    "\n",
    "            # ç”¨æ›´ä½çš„å­¸ç¿’ç‡é‡æ–°ç·¨è­¯æ¨¡å‹\n",
    "            fine_tune_lr = params['learning_rate'] * FINE_TUNE_LR_MULTIPLIER\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "                loss=keras_cv.losses.FocalLoss(from_logits=False, label_smoothing=0.1), # 2. ä½¿ç”¨ Keras-CV çš„ FocalLoss\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            callbacks_stage2 = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-9),\n",
    "                ModelCheckpoint(\"model_checkpoint.keras\", monitor=\"val_loss\", save_best_only=True)\n",
    "            ]\n",
    "            history_stage2 = model.fit(\n",
    "                train_gen, epochs=STAGE_2_EPOCHS, validation_data=val_gen,\n",
    "                class_weight=class_weight, callbacks=callbacks_stage2, verbose=1\n",
    "            )\n",
    "            \n",
    "            # --- MLflow è¨˜éŒ„ ---\n",
    "            full_history = {}\n",
    "            for key in history_stage1.history.keys():\n",
    "                full_history[key] = history_stage1.history[key] + history_stage2.history[key]\n",
    "            \n",
    "            # ### å„ªåŒ– 3bï¼šè¨ˆç®—æ•´å€‹è¨“ç·´éç¨‹ä¸­çš„æœ€ä½³é©—è­‰æº–ç¢ºç‡ ###\n",
    "            # ç¢ºä¿ history å­—å…¸ä¸æ˜¯ç©ºçš„\n",
    "            best_s1_acc = max(history_stage1.history.get('val_accuracy', [0]))\n",
    "            best_s2_acc = max(history_stage2.history.get('val_accuracy', [0]))\n",
    "            current_run_best_val_accuracy = max(best_s1_acc, best_s2_acc)\n",
    "            \n",
    "            mlflow.log_metric(\"best_val_accuracy\", current_run_best_val_accuracy)\n",
    "            \n",
    "            plot_and_log_history(type('History', (), {'history': full_history})())\n",
    "            evaluate_and_log_all_reports(model, val_gen, class_labels)\n",
    "            mlflow.keras.log_model(model, \"model\")\n",
    "            \n",
    "            print(f\"âœ”ï¸ Run {run.info.run_id} å®Œæˆã€‚æœ¬è¼ªæœ€ä½³é©—è­‰æº–ç¢ºç‡: {current_run_best_val_accuracy:.4f}\")\n",
    "            \n",
    "            if current_run_best_val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = current_run_best_val_accuracy\n",
    "                best_run_id = run.info.run_id\n",
    "                print(f\"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! Model: {params['model_name']}, Run ID: {best_run_id}, Accuracy: {best_val_accuracy:.4f}\")\n",
    "                model.save(f'checkpoints/bestmodel/{i+1}{params['model_name']}_{best_val_accuracy:.4f}.keras')\n",
    "                print(f\"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²æ›´æ–°ä¸¦å„²å­˜ç‚º '{i+1}{params['model_name']}_{best_val_accuracy:.4f}.keras'\")\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸ è‡ªå‹•èª¿åƒå®Œæˆã€‚\")\n",
    "    if best_run_id:\n",
    "        print(f\"ğŸ† æœ€çµ‚æœ€ä½³æ¨¡å‹çš„ Run ID ç‚º: {best_run_id}\")\n",
    "        print(f\"ğŸ† æœ€çµ‚æœ€ä½³é©—è­‰æº–ç¢ºç‡ç‚º: {best_val_accuracy:.4f}\")\n",
    "    else:\n",
    "        print(\"âŒ æœªèƒ½æˆåŠŸè¨“ç·´ä»»ä½•æ¨¡å‹ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c9c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸš€ Run 1/6: å˜—è©¦åƒæ•¸çµ„åˆ: {'model_name': 'MobileNetV3-Large', 'learning_rate': 0.001, 'dropout_rate': 0.3, 'dense_units': 256, 'trainable_layers': 80}\n",
      "Found 36680 images belonging to 60 classes.\n",
      "Found 7200 images belonging to 60 classes.\n",
      "\n",
      "-------------------- STAGE 1: Feature Extraction --------------------\n",
      "Epoch 1/20\n",
      "\u001b[1m1147/1147\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.2912 - loss: 0.4045"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 3. åŸ·è¡Œä¸»æµç¨‹ (å·²æ›´æ–°)\n",
    "\n",
    "# --- å®šç¾©è·¯å¾‘ ---\n",
    "train_path = \"dataset_full_en_aug6_60/train\"\n",
    "validation_path = \"dataset_full_en_aug6_60/validation\"\n",
    "\n",
    "# --- å•Ÿå‹•è‡ªå‹•è¨“ç·´èˆ‡èª¿åƒ ---\n",
    "if __name__ == '__main__':\n",
    "    # å‡½å¼å…§éƒ¨æœƒè™•ç†æ•¸æ“šç”Ÿæˆå™¨ã€é¡åˆ¥æ•¸é‡å’Œæ¬Šé‡è¨ˆç®—\n",
    "    train_and_tune(\n",
    "        train_path=train_path,\n",
    "        validation_path=validation_path,\n",
    "        target_accuracy=0.98\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef659344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
