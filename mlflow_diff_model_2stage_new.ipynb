{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ad028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "from PIL import Image \n",
    "from pathlib import Path\n",
    "import visualkeras\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from tensorflow.keras.applications import mobilenet_v3, efficientnet\n",
    "import itertools \n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import keras_cv\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers, layers, models\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint \n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 1. 建立模型的函式 (最終穩健版：繞過 Keras 3 的 input_tensor Bug)\n",
    "def build_model(model_name, learning_rate, dropout_rate1, dropout_rate2, dense_units, trainable_layers, num_classes, \n",
    "                freeze_base_model=False, loss_function='categorical_crossentropy'):\n",
    "    \"\"\"\n",
    "    根據傳入的超參數建立並編譯指定的 Keras 模型。\n",
    "    *** 修正：改變模型構建方式，以避開 Keras 3 在使用 input_tensor 時的 shape inference bug ***\n",
    "    \"\"\"\n",
    "    input_shape = (224, 224, 3) \n",
    "    \n",
    "    # --- 步驟 1：像之前一樣，定義輸入和數據增強層 ---\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    \n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(0.15),\n",
    "            layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "            layers.RandomErasing(scale=(0.02, 0.1), value_range=[0, 255]),\n",
    "        ],\n",
    "        name=\"data_augmentation\",\n",
    "    )\n",
    "\n",
    "    # --- 步驟 2：關鍵修改 - 先獨立創建基礎模型，不傳入 input_tensor ---\n",
    "    if model_name == 'MobileNetV3-Large':\n",
    "        # 注意：我們不再使用 input_tensor 參數\n",
    "        base_model = keras.applications.MobileNetV3Large(\n",
    "            input_shape=input_shape, include_top=False, weights='imagenet'\n",
    "        )\n",
    "    elif model_name == 'EfficientNet-B0':\n",
    "        # 注意：我們不再使用 input_tensor 參數\n",
    "        base_model = keras.applications.EfficientNetB0(\n",
    "            input_shape=input_shape, include_top=False, weights='imagenet'\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"不支援的模型名稱: {model_name}。\")\n",
    "        \n",
    "    # --- 步驟 3：設定基礎模型的可訓練性 ---\n",
    "    if freeze_base_model:\n",
    "        base_model.trainable = False\n",
    "    else:\n",
    "        base_model.trainable = True\n",
    "\n",
    "    # --- 步驟 4：手動連接計算圖 (Graph) ---\n",
    "    # 1. 原始輸入 -> 數據增強\n",
    "    x = data_augmentation(inputs)\n",
    "    \n",
    "    # 2. 數據增強的輸出 -> 基礎模型\n",
    "    #    在凍結階段，BN層應在推斷模式下運行，所以 training=False\n",
    "    x = base_model(x, training=False) \n",
    "    \n",
    "    # 3. 基礎模型的輸出 -> 全局池化和分類頭\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate1)(x)\n",
    "    x = layers.Dense(dense_units, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate2)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # --- 步驟 5：創建最終模型 ---\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # --- 步驟 6：編譯模型 ---\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"categorical_crossentropy\", \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b38f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    classification_report,\n",
    "    precision_recall_curve, \n",
    "    average_precision_score, \n",
    "    roc_curve, \n",
    "    auc\n",
    ")\n",
    "from itertools import cycle\n",
    "\n",
    "# 1. MLflow 視覺化與日誌記錄輔助函式 (維持不變)\n",
    "def plot_and_log_history(history, filename=\"history_plots.png\"):\n",
    "    \"\"\"\n",
    "    繪製準確率和損失函數的歷史曲線，並將其儲存為圖片，最後記錄到 MLflow。\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 準確率圖\n",
    "    ax1.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.set_title('Accuracy over epochs')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 損失函數圖\n",
    "    ax2.plot(history.history['loss'], label='Train Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Val Loss')\n",
    "    ax2.set_title('Loss over epochs')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # 儲存圖片並關閉繪圖\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 將圖片記錄到 MLflow\n",
    "    mlflow.log_artifact(filename)\n",
    "    print(f\"✅ '{filename}' has been logged to MLflow.\")\n",
    "\n",
    "def log_confusion_matrix(y_true, y_pred, class_labels, filename=\"confusion_matrix.png\"):\n",
    "    \"\"\" 繪製、儲存並記錄標準混淆矩陣 \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(18, 14))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels, cmap='Blues')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(filename)\n",
    "    print(f\"✅ '{filename}' has been logged to MLflow.\")\n",
    "\n",
    "def plot_and_log_sorted_confusion_bar(y_true, y_pred, class_labels, filename=\"sorted_correct_counts.png\"):\n",
    "    \"\"\" 計算、繪製並記錄一個長條圖，顯示每個類別的正確預測數量（由高到低排序）。 \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    correct_counts = np.diag(cm)\n",
    "    \n",
    "    sorted_idx = np.argsort(correct_counts)[::-1]\n",
    "    sorted_counts = correct_counts[sorted_idx]\n",
    "    sorted_labels = np.array(class_labels)[sorted_idx]\n",
    "\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.bar(range(len(sorted_counts)), sorted_counts, color='lightgreen')\n",
    "    plt.xticks(range(len(sorted_labels)), sorted_labels, rotation=90)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Correctly Predicted Count')\n",
    "    plt.title('Correctly Predicted Count per Class (sorted)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "    mlflow.log_artifact(filename)\n",
    "    print(f\"✅ '{filename}' has been logged to MLflow.\")\n",
    "\n",
    "# 2. 【已修正】的 PR 和 ROC 曲線函式\n",
    "def plot_and_log_precision_recall_curves(y_true, y_probas, class_labels, filename=\"precision_recall_curves.png\"):\n",
    "    \"\"\"\n",
    "    為每個類別繪製 Precision-Recall 曲線。\n",
    "    特別突顯表現最差的5個類別，並將圖表記錄到 MLflow。\n",
    "    \"\"\"\n",
    "    # 步驟 1: 計算每個類別的 Average Precision (AP) 分數\n",
    "    class_aps = []\n",
    "    for i in range(len(class_labels)):\n",
    "        ap = average_precision_score(y_true == i, y_probas[:, i])\n",
    "        class_aps.append((ap, i))\n",
    "\n",
    "    # 步驟 2: 根據 AP 分數進行排序（由低到高），找出最差的5個\n",
    "    class_aps.sort(key=lambda x: x[0])\n",
    "    worst_classes_indices = {idx for _, idx in class_aps[:5]}\n",
    "\n",
    "    # 步驟 3: 繪圖\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal']) # 為最差的5個類別準備顏色\n",
    "\n",
    "    # 繪製所有類別的曲線\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        precision, recall, _ = precision_recall_curve(y_true == i, y_probas[:, i])\n",
    "        ap_score = class_aps[i][0] if class_aps[i][1] == i else [s for s, j in class_aps if j == i][0] # 找到對應的AP\n",
    "\n",
    "        if i in worst_classes_indices:\n",
    "            # 如果是表現最差的類別之一，使用彩色線條並加入圖例\n",
    "            plt.plot(recall, precision, lw=2.5, color=next(colors),\n",
    "                     label=f'PR curve for {class_label} (AP = {ap_score:0.3f})')\n",
    "        else:\n",
    "            # 其他類別使用灰色線條，不加入圖例\n",
    "            plt.plot(recall, precision, lw=1.5, color='lightgray')\n",
    "\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Multi-class Precision-Recall Curve (Highlighting 5 worst classes)\")\n",
    "    plt.legend(loc=\"best\") # 顯示圖例\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(filename)\n",
    "    print(f\"✅ '{filename}' has been logged to MLflow.\")\n",
    "\n",
    "def plot_and_log_roc_curves(y_true, y_probas, class_labels, filename=\"roc_curves.png\"):\n",
    "    \"\"\"\n",
    "    為每個類別繪製 ROC 曲線。\n",
    "    特別突顯表現最差的5個類別，並將圖表記錄到 MLflow。\n",
    "    \"\"\"\n",
    "    # 步驟 1: 計算每個類別的 AUC 分數\n",
    "    class_aucs = []\n",
    "    for i in range(len(class_labels)):\n",
    "        fpr, tpr, _ = roc_curve(y_true == i, y_probas[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        class_aucs.append((roc_auc, i))\n",
    "    \n",
    "    # 步驟 2: 根據 AUC 分數進行排序（由低到高），找出最差的5個\n",
    "    class_aucs.sort(key=lambda x: x[0])\n",
    "    worst_classes_indices = {idx for _, idx in class_aucs[:5]}\n",
    "    \n",
    "    # 步驟 3: 繪圖\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal']) # 為最差的5個類別準備顏色\n",
    "\n",
    "    # 繪製所有類別的曲線\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        fpr, tpr, _ = roc_curve(y_true == i, y_probas[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        if i in worst_classes_indices:\n",
    "            # 如果是表現最差的類別之一，使用彩色線條並加入圖例\n",
    "            plt.plot(fpr, tpr, lw=2.5, color=next(colors),\n",
    "                     label=f'ROC curve for {class_label} (AUC = {roc_auc:0.3f})')\n",
    "        else:\n",
    "            # 其他類別使用灰色線條，不加入圖例\n",
    "            plt.plot(fpr, tpr, lw=1.5, color='lightgray')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Guess')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Multi-class ROC Curve (Highlighting 5 worst classes)\")\n",
    "    plt.legend(loc=\"lower right\") # 顯示圖例\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(filename)\n",
    "    print(f\"✅ '{filename}' has been logged to MLflow.\")\n",
    "\n",
    "\n",
    "# 3. 更新後的主評估函式\n",
    "def evaluate_and_log_all_reports(model, data_generator, class_labels):\n",
    "    \"\"\"\n",
    "    評估模型並記錄分類報告、混淆矩陣及其他分析圖（包括 PR 和 ROC 曲線）到 MLflow。\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*20 + \" Starting Full Evaluation for Logging \" + \"=\"*20)\n",
    "\n",
    "    data_generator.reset()\n",
    "\n",
    "    y_probas = model.predict(data_generator, steps=len(data_generator), verbose=1)\n",
    "    y_pred = np.argmax(y_probas, axis=1)\n",
    "    y_true = data_generator.classes\n",
    "    \n",
    "    if len(y_pred) != len(y_true):\n",
    "        print(f\"⚠️ y_pred length {len(y_pred)} vs y_true length {len(y_true)}. This is normal if batch_size doesn't divide total samples.\")\n",
    "        y_true = y_true[:len(y_pred)]\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=class_labels, digits=4, zero_division=0)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "    mlflow.log_text(report, \"classification_report.txt\")\n",
    "    print(\"✅ 'classification_report.txt' has been logged to MLflow.\")\n",
    "\n",
    "    log_confusion_matrix(y_true, y_pred, class_labels)\n",
    "    plot_and_log_sorted_confusion_bar(y_true, y_pred, class_labels)\n",
    "    plot_and_log_precision_recall_curves(y_true, y_probas, class_labels)\n",
    "    plot_and_log_roc_curves(y_true, y_probas, class_labels)\n",
    "    \n",
    "    print(\"=\"*20 + \" Full Evaluation Logging Complete \" + \"=\"*20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ad3f6",
   "metadata": {},
   "source": [
    "### 兩種模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b30e6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 2. 自動訓練與調參的主函式 (全面優化版)\n",
    "def train_and_tune(train_path, validation_path, target_accuracy):\n",
    "    \"\"\"\n",
    "    自動執行兩階段訓練和超參數調整。\n",
    "    實作了多項遷移學習最佳實踐。\n",
    "    \"\"\"\n",
    "    param_space = {\n",
    "        'model_name': [\n",
    "            'MobileNetV3-Large', \n",
    "            # 'EfficientNet-B0'\n",
    "        ],\n",
    "        'learning_rate': [0.001],\n",
    "        'dropout_rate1': [0.5],\n",
    "        'dropout_rate2': [0.3],\n",
    "        'dense_units': [256],\n",
    "        'trainable_layers': [100],\n",
    "\n",
    "        # 'learning_rate': [0.001],\n",
    "        # 'dropout_rate': [0.3],\n",
    "        # 'dense_units': [256],\n",
    "        # 'trainable_layers': [80]\n",
    "    }\n",
    "    \n",
    "    STAGE_1_EPOCHS = 30\n",
    "    STAGE_2_EPOCHS = 50\n",
    "    FINE_TUNE_LR_MULTIPLIER = 0.1\n",
    "    \n",
    "    keys, values = zip(*param_space.items())\n",
    "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    best_run_id = None\n",
    "\n",
    "    mlflow.set_experiment(\"MobileNetV3新2階段 RandomErasing 60種(資料夾7)\")\n",
    "    \n",
    "    for i, params in enumerate(param_combinations):\n",
    "\n",
    "        if best_val_accuracy >= target_accuracy:\n",
    "            print(f\"🎯 目標已達成 (Accuracy >= {target_accuracy:.2f})。停止搜尋。\")\n",
    "            break\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"🚀 Run {i+1}/{len(param_combinations)}: 嘗試參數組合: {params}\")\n",
    "\n",
    "        if params['model_name'] == 'MobileNetV3-Large':\n",
    "            preprocessing_function = mobilenet_v3.preprocess_input\n",
    "        elif params['model_name'] == 'EfficientNet-B0':\n",
    "            preprocessing_function = efficientnet.preprocess_input\n",
    "        else:\n",
    "            raise ValueError(f\"未知的模型: {params['model_name']}\")\n",
    "\n",
    "        # train_datagen_aug = ImageDataGenerator(\n",
    "        #     preprocessing_function=preprocessing_function,\n",
    "        #     rotation_range=15,\n",
    "        #     width_shift_range=0.1,\n",
    "        #     height_shift_range=0.1,\n",
    "        #     zoom_range=0.1,\n",
    "        #     horizontal_flip=True,\n",
    "        #     # ### 優化 2：為避免數據分布偏移，建議優先移除亮度增強 ###\n",
    "        #     # brightness_range=[0.9, 1.1],\n",
    "        #     fill_mode='nearest'\n",
    "        # )\n",
    "\n",
    "        train_datagen_simple  = ImageDataGenerator(preprocessing_function=preprocessing_function)\n",
    "\n",
    "        validation_datagen = ImageDataGenerator(preprocessing_function=preprocessing_function)\n",
    "\n",
    "        train_gen = train_datagen_simple.flow_from_directory(\n",
    "            train_path, target_size=(224, 224), batch_size=32, class_mode='categorical', shuffle=True\n",
    "        )\n",
    "        val_gen = validation_datagen.flow_from_directory(\n",
    "            validation_path, target_size=(224, 224), batch_size=32, class_mode='categorical', shuffle=False\n",
    "        )\n",
    "\n",
    "        num_classes = train_gen.num_classes\n",
    "        class_labels = list(train_gen.class_indices.keys())\n",
    "\n",
    "        with open('classes_new.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for cls in class_labels:\n",
    "                writer.writerow([cls])\n",
    "        \n",
    "        class_indices = np.unique(train_gen.classes)\n",
    "        class_weights_array = compute_class_weight('balanced', classes=class_indices, y=train_gen.classes)\n",
    "        class_weight = dict(zip(class_indices, class_weights_array))\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"{params['model_name']}_run_{i+1}\") as run:\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "            # STAGE 1: Feature Extraction\n",
    "            print(\"\\n\" + \"-\" * 20 + \" STAGE 1: Feature Extraction \" + \"-\" * 20)\n",
    "            \n",
    "            # ### 優化 1：接收 model 和 base_model ###\n",
    "            model, base_model = build_model(\n",
    "                **params, num_classes=num_classes, freeze_base_model=True\n",
    "            )\n",
    "            \n",
    "            history_stage1 = model.fit(\n",
    "                train_gen, epochs=STAGE_1_EPOCHS, validation_data=val_gen,\n",
    "                callbacks=[EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)],\n",
    "                class_weight=class_weight, verbose=1\n",
    "            )\n",
    "            \n",
    "            # STAGE 2: Fine-Tuning\n",
    "            print(\"\\n\" + \"-\" * 20 + \" STAGE 2: Fine-Tuning \" + \"-\" * 20)\n",
    "\n",
    "            # --- 直接在現有 model 物件上操作 ---\n",
    "            base_model.trainable = True\n",
    "            \n",
    "            trainable_layers_count = params['trainable_layers']\n",
    "            if trainable_layers_count > 0 and trainable_layers_count < len(base_model.layers):\n",
    "                print(f\"🔥 Fine-tuning: Unfreezing the last {trainable_layers_count} layers.\")\n",
    "                for layer in base_model.layers[:-trainable_layers_count]:\n",
    "                    layer.trainable = False\n",
    "            else:\n",
    "                print(\"🔥 Fine-tuning: Unfreezing all base model layers.\")\n",
    "            \n",
    "            # ### 優化 3a：在微調時凍結 BatchNormalization 層 ###\n",
    "            for layer in base_model.layers:\n",
    "                if isinstance(layer, layers.BatchNormalization):\n",
    "                    layer.trainable = False\n",
    "            print(\"🧊 All BatchNormalization layers in the base model have been frozen.\")\n",
    "\n",
    "            # 用更低的學習率重新編譯模型\n",
    "            fine_tune_lr = params['learning_rate'] * FINE_TUNE_LR_MULTIPLIER\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "                loss=\"categorical_crossentropy\",\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            callbacks_stage2 = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-9),\n",
    "                ModelCheckpoint(\"model_checkpoint.keras\", monitor=\"val_loss\", save_best_only=True)\n",
    "            ]\n",
    "            history_stage2 = model.fit(\n",
    "                train_gen, epochs=STAGE_2_EPOCHS, validation_data=val_gen,\n",
    "                class_weight=class_weight, callbacks=callbacks_stage2, verbose=1\n",
    "            )\n",
    "            \n",
    "            # --- MLflow 記錄 ---\n",
    "            full_history = {}\n",
    "            for key in history_stage1.history.keys():\n",
    "                full_history[key] = history_stage1.history[key] + history_stage2.history[key]\n",
    "            \n",
    "            # ### 優化 3b：計算整個訓練過程中的最佳驗證準確率 ###\n",
    "            # 確保 history 字典不是空的\n",
    "            best_s1_acc = max(history_stage1.history.get('val_accuracy', [0]))\n",
    "            best_s2_acc = max(history_stage2.history.get('val_accuracy', [0]))\n",
    "            current_run_best_val_accuracy = max(best_s1_acc, best_s2_acc)\n",
    "            \n",
    "            mlflow.log_metric(\"best_val_accuracy\", current_run_best_val_accuracy)\n",
    "            \n",
    "            plot_and_log_history(type('History', (), {'history': full_history})())\n",
    "            evaluate_and_log_all_reports(model, val_gen, class_labels)\n",
    "            mlflow.keras.log_model(model, \"model\")\n",
    "            \n",
    "            print(f\"✔️ Run {run.info.run_id} 完成。本輪最佳驗證準確率: {current_run_best_val_accuracy:.4f}\")\n",
    "            \n",
    "            if current_run_best_val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = current_run_best_val_accuracy\n",
    "                best_run_id = run.info.run_id\n",
    "                print(f\"🎉 新的最佳模型! Model: {params['model_name']}, Run ID: {best_run_id}, Accuracy: {best_val_accuracy:.4f}\")\n",
    "                model.save(f'checkpoints/bestmodel/{i+1}{params['model_name']}_{best_val_accuracy:.4f}.keras')\n",
    "                print(f\"💾 最佳模型已更新並儲存為 '{i+1}{params['model_name']}_{best_val_accuracy:.4f}.keras'\")\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"🏁 自動調參完成。\")\n",
    "    if best_run_id:\n",
    "        print(f\"🏆 最終最佳模型的 Run ID 為: {best_run_id}\")\n",
    "        print(f\"🏆 最終最佳驗證準確率為: {best_val_accuracy:.4f}\")\n",
    "    else:\n",
    "        print(\"❌ 未能成功訓練任何模型。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e09c9c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 22:36:33 INFO mlflow.tracking.fluent: Experiment with name 'MobileNetV3新2階段 RandomErasing 60種(資料夾7)' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "🚀 Run 1/1: 嘗試參數組合: {'model_name': 'MobileNetV3-Large', 'learning_rate': 0.001, 'dropout_rate1': 0.5, 'dropout_rate2': 0.3, 'dense_units': 256, 'trainable_layers': 100}\n",
      "Found 34279 images belonging to 56 classes.\n",
      "Found 6720 images belonging to 56 classes.\n",
      "\n",
      "-------------------- STAGE 1: Feature Extraction --------------------\n",
      "Epoch 1/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 431ms/step - accuracy: 0.4387 - loss: 1.9668 - val_accuracy: 0.7189 - val_loss: 0.9160\n",
      "Epoch 2/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 363ms/step - accuracy: 0.6077 - loss: 1.2930 - val_accuracy: 0.7574 - val_loss: 0.7851\n",
      "Epoch 3/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 367ms/step - accuracy: 0.6589 - loss: 1.1245 - val_accuracy: 0.7756 - val_loss: 0.7239\n",
      "Epoch 4/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 363ms/step - accuracy: 0.6807 - loss: 1.0444 - val_accuracy: 0.7923 - val_loss: 0.6970\n",
      "Epoch 5/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 359ms/step - accuracy: 0.6994 - loss: 0.9896 - val_accuracy: 0.7897 - val_loss: 0.6736\n",
      "Epoch 6/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 365ms/step - accuracy: 0.7104 - loss: 0.9465 - val_accuracy: 0.8089 - val_loss: 0.6433\n",
      "Epoch 7/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 359ms/step - accuracy: 0.7169 - loss: 0.9172 - val_accuracy: 0.8106 - val_loss: 0.6462\n",
      "Epoch 8/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 366ms/step - accuracy: 0.7304 - loss: 0.8829 - val_accuracy: 0.8073 - val_loss: 0.6503\n",
      "Epoch 9/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 392ms/step - accuracy: 0.7362 - loss: 0.8640 - val_accuracy: 0.8155 - val_loss: 0.6257\n",
      "Epoch 10/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 478ms/step - accuracy: 0.7419 - loss: 0.8498 - val_accuracy: 0.8161 - val_loss: 0.6339\n",
      "Epoch 11/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 500ms/step - accuracy: 0.7489 - loss: 0.8203 - val_accuracy: 0.8207 - val_loss: 0.6031\n",
      "Epoch 12/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 501ms/step - accuracy: 0.7476 - loss: 0.8278 - val_accuracy: 0.8192 - val_loss: 0.6025\n",
      "Epoch 13/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 496ms/step - accuracy: 0.7584 - loss: 0.7877 - val_accuracy: 0.8180 - val_loss: 0.6085\n",
      "Epoch 14/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 491ms/step - accuracy: 0.7636 - loss: 0.7854 - val_accuracy: 0.8193 - val_loss: 0.5988\n",
      "Epoch 15/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 500ms/step - accuracy: 0.7674 - loss: 0.7800 - val_accuracy: 0.8231 - val_loss: 0.6072\n",
      "Epoch 16/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 499ms/step - accuracy: 0.7684 - loss: 0.7628 - val_accuracy: 0.8266 - val_loss: 0.5867\n",
      "Epoch 17/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 504ms/step - accuracy: 0.7723 - loss: 0.7564 - val_accuracy: 0.8266 - val_loss: 0.5978\n",
      "Epoch 18/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m539s\u001b[0m 503ms/step - accuracy: 0.7747 - loss: 0.7417 - val_accuracy: 0.8188 - val_loss: 0.6158\n",
      "Epoch 19/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 502ms/step - accuracy: 0.7760 - loss: 0.7475 - val_accuracy: 0.8317 - val_loss: 0.5849\n",
      "Epoch 20/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 494ms/step - accuracy: 0.7795 - loss: 0.7241 - val_accuracy: 0.8287 - val_loss: 0.5905\n",
      "Epoch 21/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m552s\u001b[0m 485ms/step - accuracy: 0.7826 - loss: 0.7185 - val_accuracy: 0.8214 - val_loss: 0.6172\n",
      "Epoch 22/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m516s\u001b[0m 481ms/step - accuracy: 0.7863 - loss: 0.7207 - val_accuracy: 0.8342 - val_loss: 0.5854\n",
      "Epoch 23/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m512s\u001b[0m 478ms/step - accuracy: 0.7858 - loss: 0.7108 - val_accuracy: 0.8268 - val_loss: 0.5951\n",
      "Epoch 24/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 501ms/step - accuracy: 0.7843 - loss: 0.7172 - val_accuracy: 0.8336 - val_loss: 0.5902\n",
      "Epoch 25/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 498ms/step - accuracy: 0.7872 - loss: 0.7042 - val_accuracy: 0.8305 - val_loss: 0.6002\n",
      "Epoch 26/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m522s\u001b[0m 487ms/step - accuracy: 0.7868 - loss: 0.7054 - val_accuracy: 0.8362 - val_loss: 0.5761\n",
      "Epoch 27/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 492ms/step - accuracy: 0.7952 - loss: 0.6832 - val_accuracy: 0.8277 - val_loss: 0.5907\n",
      "Epoch 28/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m575s\u001b[0m 505ms/step - accuracy: 0.7923 - loss: 0.6947 - val_accuracy: 0.8302 - val_loss: 0.5894\n",
      "Epoch 29/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 502ms/step - accuracy: 0.7896 - loss: 0.6990 - val_accuracy: 0.8290 - val_loss: 0.6044\n",
      "Epoch 30/30\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m543s\u001b[0m 507ms/step - accuracy: 0.7932 - loss: 0.6944 - val_accuracy: 0.8313 - val_loss: 0.6016\n",
      "\n",
      "-------------------- STAGE 2: Fine-Tuning --------------------\n",
      "🔥 Fine-tuning: Unfreezing the last 100 layers.\n",
      "🧊 All BatchNormalization layers in the base model have been frozen.\n",
      "Epoch 1/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m814s\u001b[0m 745ms/step - accuracy: 0.8348 - loss: 0.5496 - val_accuracy: 0.8622 - val_loss: 0.4922 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 709ms/step - accuracy: 0.8712 - loss: 0.4200 - val_accuracy: 0.8705 - val_loss: 0.4464 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m753s\u001b[0m 702ms/step - accuracy: 0.8887 - loss: 0.3650 - val_accuracy: 0.8763 - val_loss: 0.4529 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m758s\u001b[0m 707ms/step - accuracy: 0.9029 - loss: 0.3177 - val_accuracy: 0.8856 - val_loss: 0.4286 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m797s\u001b[0m 703ms/step - accuracy: 0.9126 - loss: 0.2896 - val_accuracy: 0.8893 - val_loss: 0.4117 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m750s\u001b[0m 700ms/step - accuracy: 0.9230 - loss: 0.2559 - val_accuracy: 0.8993 - val_loss: 0.3846 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 709ms/step - accuracy: 0.9311 - loss: 0.2296 - val_accuracy: 0.8909 - val_loss: 0.4049 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m766s\u001b[0m 715ms/step - accuracy: 0.9360 - loss: 0.2070 - val_accuracy: 0.8981 - val_loss: 0.3921 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m764s\u001b[0m 713ms/step - accuracy: 0.9382 - loss: 0.2032 - val_accuracy: 0.9045 - val_loss: 0.3955 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m806s\u001b[0m 717ms/step - accuracy: 0.9608 - loss: 0.1241 - val_accuracy: 0.9128 - val_loss: 0.3617 - learning_rate: 2.0000e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m799s\u001b[0m 714ms/step - accuracy: 0.9664 - loss: 0.1081 - val_accuracy: 0.9176 - val_loss: 0.3534 - learning_rate: 2.0000e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m780s\u001b[0m 727ms/step - accuracy: 0.9679 - loss: 0.1019 - val_accuracy: 0.9179 - val_loss: 0.3611 - learning_rate: 2.0000e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m804s\u001b[0m 729ms/step - accuracy: 0.9688 - loss: 0.0997 - val_accuracy: 0.9162 - val_loss: 0.3641 - learning_rate: 2.0000e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m786s\u001b[0m 733ms/step - accuracy: 0.9722 - loss: 0.0872 - val_accuracy: 0.9153 - val_loss: 0.3657 - learning_rate: 2.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m788s\u001b[0m 736ms/step - accuracy: 0.9741 - loss: 0.0812 - val_accuracy: 0.9170 - val_loss: 0.3665 - learning_rate: 4.0000e-06\n",
      "Epoch 16/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m789s\u001b[0m 736ms/step - accuracy: 0.9752 - loss: 0.0787 - val_accuracy: 0.9176 - val_loss: 0.3714 - learning_rate: 4.0000e-06\n",
      "Epoch 17/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m798s\u001b[0m 744ms/step - accuracy: 0.9748 - loss: 0.0779 - val_accuracy: 0.9168 - val_loss: 0.3680 - learning_rate: 4.0000e-06\n",
      "Epoch 18/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m806s\u001b[0m 748ms/step - accuracy: 0.9752 - loss: 0.0791 - val_accuracy: 0.9165 - val_loss: 0.3660 - learning_rate: 8.0000e-07\n",
      "Epoch 19/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m806s\u001b[0m 752ms/step - accuracy: 0.9764 - loss: 0.0752 - val_accuracy: 0.9165 - val_loss: 0.3656 - learning_rate: 8.0000e-07\n",
      "Epoch 20/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m867s\u001b[0m 756ms/step - accuracy: 0.9766 - loss: 0.0742 - val_accuracy: 0.9176 - val_loss: 0.3664 - learning_rate: 8.0000e-07\n",
      "Epoch 21/50\n",
      "\u001b[1m1072/1072\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m811s\u001b[0m 756ms/step - accuracy: 0.9774 - loss: 0.0720 - val_accuracy: 0.9177 - val_loss: 0.3663 - learning_rate: 1.6000e-07\n",
      "✅ 'history_plots.png' has been logged to MLflow.\n",
      "\n",
      "==================== Starting Full Evaluation for Logging ====================\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 390ms/step\n",
      "\n",
      "Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "Agaricus lemaneiformis     0.9407    0.9250    0.9328       120\n",
      "              Amaranth     0.8934    0.9083    0.9008       120\n",
      "             Baby Corn     0.9083    0.9083    0.9083       120\n",
      "         Bamboo shoots     0.9120    0.9500    0.9306       120\n",
      "                 Basil     0.8583    0.9083    0.8826       120\n",
      "           Beef Tomato     0.9587    0.9667    0.9627       120\n",
      "           Bell pepper     0.9583    0.9583    0.9583       120\n",
      "   Big Chinese Cabbage     0.9350    0.9583    0.9465       120\n",
      "          Big cucumber     0.9459    0.8750    0.9091       120\n",
      "              Bok Choy     0.9328    0.9250    0.9289       120\n",
      "       Chinese Cabbage     0.9083    0.9083    0.9083       120\n",
      "        Chinese chives     0.8583    0.9083    0.8826       120\n",
      "         Chrysanthemum     0.9076    0.9000    0.9038       120\n",
      "          French beans     0.7338    0.8500    0.7876       120\n",
      "                Garlic     0.9667    0.9667    0.9667       120\n",
      "        Green Broccoli     0.9587    0.9667    0.9627       120\n",
      "   Green bamboo shoots     0.9835    0.9917    0.9876       120\n",
      "          Green pepper     0.9912    0.9417    0.9658       120\n",
      "                  Kale     0.8430    0.8500    0.8465       120\n",
      "               Lettuce     0.9339    0.9417    0.9378       120\n",
      "                Loofah     0.9292    0.8750    0.9013       120\n",
      "            Lotus root     0.9735    0.9167    0.9442       120\n",
      "         Mainland girl     0.8571    0.9500    0.9012       120\n",
      "   Momordica charantia     0.9512    0.9750    0.9630       120\n",
      "           Mountain Su     0.9732    0.9083    0.9397       120\n",
      "                  Okra     0.8421    0.9333    0.8854       120\n",
      "          Red broccoli     0.9369    0.8667    0.9004       120\n",
      "               Romaine     0.9722    0.8750    0.9211       120\n",
      "              Shallots     0.9750    0.9750    0.9750       120\n",
      "             Sweet Pea     0.9279    0.8583    0.8918       120\n",
      "   Sweet potato leaves     0.8074    0.9083    0.8549       120\n",
      "                  Taro     0.9440    0.9833    0.9633       120\n",
      "            Water Lily     0.9832    0.9750    0.9791       120\n",
      "         Water spinach     0.7899    0.9083    0.8450       120\n",
      "          White radish     0.8741    0.9833    0.9255       120\n",
      "          Winter melon     0.9407    0.9250    0.9328       120\n",
      "                   Yam     0.9794    0.7917    0.8756       120\n",
      "             asparagus     0.8760    0.9417    0.9076       120\n",
      "               brocoli     0.9831    0.9667    0.9748       120\n",
      "               cabbage     0.9402    0.9167    0.9283       120\n",
      "                carrot     0.9583    0.9583    0.9583       120\n",
      "                celery     0.8981    0.8083    0.8509       120\n",
      "                 chili     0.9664    0.9583    0.9623       120\n",
      "             coriander     0.8231    0.8917    0.8560       120\n",
      "                  corn     0.9174    0.9250    0.9212       120\n",
      "                cowpea     0.9266    0.8417    0.8821       120\n",
      "              eggplant     0.9664    0.9583    0.9623       120\n",
      "                ginger     0.9593    0.9833    0.9712       120\n",
      "           green onion     0.9712    0.8417    0.9018       120\n",
      "                 onion     0.9504    0.9583    0.9544       120\n",
      "                potato     0.9244    0.9167    0.9205       120\n",
      "               pumpkin     0.9550    0.8833    0.9177       120\n",
      "                  rape     0.8609    0.8250    0.8426       120\n",
      "               spinach     0.8981    0.8083    0.8509       120\n",
      "          sweet potato     0.8692    0.9417    0.9040       120\n",
      "              zucchini     0.8968    0.9417    0.9187       120\n",
      "\n",
      "              accuracy                         0.9176      6720\n",
      "             macro avg     0.9201    0.9176    0.9178      6720\n",
      "          weighted avg     0.9201    0.9176    0.9178      6720\n",
      "\n",
      "✅ 'classification_report.txt' has been logged to MLflow.\n",
      "✅ 'confusion_matrix.png' has been logged to MLflow.\n",
      "✅ 'sorted_correct_counts.png' has been logged to MLflow.\n",
      "✅ 'precision_recall_curves.png' has been logged to MLflow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/25 07:21:35 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'roc_curves.png' has been logged to MLflow.\n",
      "==================== Full Evaluation Logging Complete ====================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/25 07:21:35 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "2025/08/25 07:22:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Run b54019b8cb5b45148a17f75963967e55 完成。本輪最佳驗證準確率: 0.9179\n",
      "🎉 新的最佳模型! Model: MobileNetV3-Large, Run ID: b54019b8cb5b45148a17f75963967e55, Accuracy: 0.9179\n",
      "💾 最佳模型已更新並儲存為 '1MobileNetV3-Large_0.9179.keras'\n",
      "======================================================================\n",
      "🏁 自動調參完成。\n",
      "🏆 最終最佳模型的 Run ID 為: b54019b8cb5b45148a17f75963967e55\n",
      "🏆 最終最佳驗證準確率為: 0.9179\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 3. 執行主流程 (已更新)\n",
    "\n",
    "# --- 定義路徑 ---\n",
    "train_path = \"dataset_full_en_aug7_56_new/train\"\n",
    "validation_path = \"dataset_full_en_aug7_56_new/validation\"\n",
    "\n",
    "# --- 啟動自動訓練與調參 ---\n",
    "if __name__ == '__main__':\n",
    "    # 函式內部會處理數據生成器、類別數量和權重計算\n",
    "    train_and_tune(\n",
    "        train_path=train_path,\n",
    "        validation_path=validation_path,\n",
    "        target_accuracy=0.98\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef659344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
